{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df17899f",
      "metadata": {
        "id": "df17899f"
      },
      "outputs": [],
      "source": [
        "# Nếu bạn chạy lại nhiều lần, cell này dọn repo cũ để tránh stale code\n",
        "%cd /content\n",
        "!rm -rf 23CLCT2_TraditionalMedicineChatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd1aef8",
      "metadata": {
        "id": "9fd1aef8"
      },
      "outputs": [],
      "source": [
        "# Clone đúng branch\n",
        "!git clone -b main --single-branch https://github.com/HuyTran28/23CLCT2_TraditionalMedicineChatbot.git\n",
        "%cd 23CLCT2_TraditionalMedicineChatbot\n",
        "\n",
        "!git rev-parse --abbrev-ref HEAD\n",
        "!git log -1 --oneline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6ea154d",
      "metadata": {
        "collapsed": true,
        "id": "b6ea154d"
      },
      "outputs": [],
      "source": [
        "# Cài dependencies\n",
        "!python -m pip install -q -r requirements.txt\n",
        "!python -m pip install -q pyngrok\n",
        "\n",
        "# Khuyến nghị cho chạy model trên GPU (device_map=auto) và 4bit (LOAD_IN_4BIT=1)\n",
        "!python -m pip install -q -U accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e98aab7",
      "metadata": {
        "id": "8e98aab7"
      },
      "source": [
        "## Cấu hình model (tuỳ chọn)\n",
        "Script server đọc các biến môi trường:\n",
        "- `HF_MODEL` hoặc `MODEL_ID` (mặc định: `Qwen/Qwen2.5-14B-Instruct`)\n",
        "- `LOAD_IN_4BIT=1` để bật 4bit nếu môi trường hỗ trợ\n",
        "- `HF_DEVICE_MAP=auto` (khuyên dùng)\n",
        "\n",
        "Nếu bạn bật `LLM_API_KEY`, local phải gửi header `Authorization: Bearer <key>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46fa2d20",
      "metadata": {
        "id": "46fa2d20"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Model\n",
        "os.environ.setdefault(\"HF_MODEL\", \"Qwen/Qwen2.5-14B-Instruct\")\n",
        "# 4bit (đặt '0' nếu muốn tắt)\n",
        "os.environ.setdefault(\"LOAD_IN_4BIT\", \"1\")\n",
        "os.environ.setdefault(\"HF_DEVICE_MAP\", \"auto\")\n",
        "\n",
        "# Bảo vệ API (tuỳ chọn). Nếu không muốn auth thì để trống / comment dòng dưới\n",
        "# os.environ[\"LLM_API_KEY\"] = \"my-secret-token\"\n",
        "\n",
        "print(\"HF_MODEL=\", os.environ.get(\"HF_MODEL\"))\n",
        "print(\"LOAD_IN_4BIT=\", os.environ.get(\"LOAD_IN_4BIT\"))\n",
        "print(\"HF_DEVICE_MAP=\", os.environ.get(\"HF_DEVICE_MAP\"))\n",
        "print(\"LLM_API_KEY set?\", bool(os.environ.get(\"LLM_API_KEY\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1e2575a",
      "metadata": {
        "id": "f1e2575a"
      },
      "source": [
        "## Chạy server nền\n",
        "Chạy nền để bạn vẫn có thể mở ngrok và test trong các cell kế tiếp.\n",
        "\n",
        "Nếu server crash (thường do thiếu VRAM hoặc lỗi tải model), cell sẽ in log để bạn xem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "529430aa",
      "metadata": {
        "id": "529430aa"
      },
      "outputs": [],
      "source": [
        "import os, subprocess, sys, time, requests\n",
        "\n",
        "# Optional: avoid noisy progress bars that can bloat logs\n",
        "os.environ.setdefault(\"HF_HUB_DISABLE_PROGRESS_BARS\", \"1\")\n",
        "\n",
        "# Optional: if you previously hit torchvision/torch mismatch in Colab, uncomment:\n",
        "!pip -q uninstall -y torchvision\n",
        "\n",
        "# IMPORTANT: don't pipe stdout to avoid deadlock when logs get large.\n",
        "# Write logs to a file instead.\n",
        "log_path = \"server.log\"\n",
        "log_fp = open(log_path, \"w\")\n",
        "\n",
        "print(\"Starting server...\")\n",
        "server_proc = subprocess.Popen(\n",
        "    [sys.executable, \"-u\", \"code/chatbot/scripts/colab_llm_server.py\"],\n",
        "    stdout=log_fp,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "# Model load can take several minutes. While loading, /health may refuse connections.\n",
        "deadline = time.time() + 900  # 15 minutes\n",
        "last_err = None\n",
        "\n",
        "print(\"Waiting for /health (this can take 5–15 minutes on first run)...\")\n",
        "while time.time() < deadline:\n",
        "    if server_proc.poll() is not None:\n",
        "        print(\"Server exited with code:\", server_proc.returncode)\n",
        "        break\n",
        "    try:\n",
        "        r = requests.get(\"http://127.0.0.1:8000/health\", timeout=2)\n",
        "        if r.status_code == 200:\n",
        "            print(\"READY:\", r.status_code, r.text)\n",
        "            break\n",
        "    except Exception as e:\n",
        "        last_err = e\n",
        "        print(\".\", end=\"\", flush=True)\n",
        "        time.sleep(5)\n",
        "else:\n",
        "    print(\"\\nTimeout waiting for /health. Last error:\", last_err)\n",
        "\n",
        "print(\"\\n\\n=== server.log (tail) ===\")\n",
        "log_fp.flush()\n",
        "try:\n",
        "    with open(log_path, \"r\") as f:\n",
        "        tail = f.readlines()[-80:]\n",
        "    print(\"\".join(tail))\n",
        "except Exception as e:\n",
        "    print(\"Could not read server.log:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a60655c",
      "metadata": {
        "id": "8a60655c"
      },
      "outputs": [],
      "source": [
        "# Test local trước khi mở ngrok\n",
        "import requests\n",
        "\n",
        "r = requests.get(\"http://127.0.0.1:8000/health\", timeout=20)\n",
        "print(\"status:\", r.status_code)\n",
        "print(\"body:\", r.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15ba4403",
      "metadata": {
        "id": "15ba4403"
      },
      "source": [
        "## Mở ngrok\n",
        "- Lấy authtoken tại https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "- Notebook sẽ ép IPv4 (`127.0.0.1:8000`) để tránh lỗi `dial tcp [::1]:8000 ... connection refused`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8344593b",
      "metadata": {
        "id": "8344593b"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the token using google.colab.userdata\n",
        "try:\n",
        "    NGROK_TOKEN = userdata.get(\"NGROK_TOKEN\")\n",
        "except Exception:\n",
        "    # Fallback or error if the secret name is wrong\n",
        "    NGROK_TOKEN = None\n",
        "\n",
        "if not NGROK_TOKEN:\n",
        "    raise RuntimeError(\"NGROK_TOKEN not found. Check that the Secret name is exactly 'NGROK_TOKEN' and that you have granted access.\")\n",
        "\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# Dọn tunnel cũ nếu có\n",
        "try:\n",
        "    ngrok.kill()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "public_url = ngrok.connect(addr=\"127.0.0.1:8000\", proto=\"http\")\n",
        "print('LLM_API_BASE =', public_url.public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff151eda",
      "metadata": {
        "id": "ff151eda"
      },
      "outputs": [],
      "source": [
        "# Test qua public URL (đừng gọi .json() ngay nếu bạn đang debug)\n",
        "import os, requests\n",
        "\n",
        "base = public_url.public_url\n",
        "\n",
        "headers = {}\n",
        "if os.getenv('LLM_API_KEY'):\n",
        "    headers['Authorization'] = f\"Bearer {os.getenv('LLM_API_KEY')}\"\n",
        "\n",
        "r = requests.get(f\"{base}/health\", timeout=30, headers=headers)\n",
        "print('health status:', r.status_code)\n",
        "print('health body:', r.text[:500])\n",
        "\n",
        "# Keep this request tiny so it's fast even on CPU/offload.\n",
        "payload = {\n",
        "    'prompt': 'Trả lời đúng 1 ký tự: 2+2=?',\n",
        "    'max_new_tokens': 4,\n",
        "    'temperature': 0.0,\n",
        "}\n",
        "r2 = requests.post(f\"{base}/v1/complete\", json=payload, timeout=300, headers=headers)\n",
        "print('complete status:', r2.status_code)\n",
        "print('complete body:', r2.text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f32440f",
      "metadata": {
        "id": "1f32440f"
      },
      "source": [
        "## Dùng URL này ở máy local (Windows PowerShell)\n",
        "Sau khi có `LLM_API_BASE`, bên Windows bạn set biến môi trường như sau:\n",
        "\n",
        "```powershell\n",
        "$env:LLM_API_BASE=\"<paste LLM_API_BASE from colab>\"\n",
        "# Nếu bạn bật auth\n",
        "$env:LLM_API_KEY=\"<same token as colab>\"\n",
        "```\n",
        "Sau đó chạy query/webapp trên máy local."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
