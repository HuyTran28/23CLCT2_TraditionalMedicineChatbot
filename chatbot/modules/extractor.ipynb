{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdd228d",
   "metadata": {},
   "source": [
    "# Colab quickstart (clone repo + cài dependencies)\n",
    "\n",
    "> Nếu bạn chạy trên **Google Colab**, hãy chạy **2 cell đầu tiên** để tự động:\n",
    "- clone repo về `/content/23CLCT2_TraditionalMedicineChatbot`\n",
    "- `pip install -r requirements.txt`\n",
    "- set `PROJECT_ROOT` và `sys.path` để import `modules.*` chạy được\n",
    "\n",
    "> Nếu bạn chạy **local VS Code/Windows**, bạn có thể bỏ qua phần clone/pip bên dưới."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a9f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto setup for Colab: clone repo + install requirements + set PROJECT_ROOT/sys.path\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ\n",
    "print('IN_COLAB:', IN_COLAB)\n",
    "\n",
    "# Repo location on Colab\n",
    "DEFAULT_PROJECT_ROOT = Path('/content/23CLCT2_TraditionalMedicineChatbot')\n",
    "PROJECT_ROOT = Path(os.environ.get('PROJECT_ROOT', str(DEFAULT_PROJECT_ROOT))).resolve()\n",
    "\n",
    "# Default repo URL (override by setting env REPO_URL if needed)\n",
    "DEFAULT_REPO_URL = 'https://github.com/HuyTran28/23CLCT2_TraditionalMedicineChatbot.git'\n",
    "REPO_URL = os.environ.get('REPO_URL', DEFAULT_REPO_URL)\n",
    "\n",
    "if IN_COLAB:\n",
    "    import subprocess\n",
    "    if not PROJECT_ROOT.exists():\n",
    "        print('PROJECT_ROOT does not exist; cloning repo...')\n",
    "        subprocess.check_call(['git', 'clone', REPO_URL, str(PROJECT_ROOT)])\n",
    "        print('Cloned to:', PROJECT_ROOT)\n",
    "    else:\n",
    "        print('Repo already exists at:', PROJECT_ROOT)\n",
    "        # Make sure repo is up-to-date (important if you edited code then re-ran on Colab)\n",
    "        try:\n",
    "            subprocess.check_call(['git', '-C', str(PROJECT_ROOT), 'pull', '--ff-only'])\n",
    "        except Exception as e:\n",
    "            print('git pull failed (non-fatal):', e)\n",
    "\n",
    "    # Print current commit (helps debugging)\n",
    "    try:\n",
    "        head = subprocess.check_output(['git', '-C', str(PROJECT_ROOT), 'rev-parse', '--short', 'HEAD']).decode().strip()\n",
    "        print('Repo HEAD:', head)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Install dependencies\n",
    "    req_file = PROJECT_ROOT / 'requirements.txt'\n",
    "    if req_file.exists():\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-U', 'pip'])\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-U', '-r', str(req_file)])\n",
    "        # Optional: for faster/larger models on Colab (4-bit)\n",
    "        # subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-U', 'accelerate', 'bitsandbytes'])\n",
    "    else:\n",
    "        raise RuntimeError(f'Không thấy requirements.txt tại: {req_file}')\n",
    "\n",
    "# Make imports work: add chatbot/ to sys.path\n",
    "chatbot_dir = (PROJECT_ROOT / 'chatbot').resolve()\n",
    "if chatbot_dir.exists():\n",
    "    if str(chatbot_dir) not in sys.path:\n",
    "        sys.path.insert(0, str(chatbot_dir))\n",
    "else:\n",
    "    # Local VS Code: fallback to current-working-directory search\n",
    "    p = Path.cwd().resolve()\n",
    "    for _ in range(6):\n",
    "        if (p / 'chatbot').exists():\n",
    "            chatbot_dir = (p / 'chatbot').resolve()\n",
    "            if str(chatbot_dir) not in sys.path:\n",
    "                sys.path.insert(0, str(chatbot_dir))\n",
    "            break\n",
    "        p = p.parent\n",
    "    else:\n",
    "        raise RuntimeError('Không tìm thấy thư mục chatbot/. Hãy mở notebook từ trong repo hoặc set env PROJECT_ROOT đúng.')\n",
    "\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "print('chatbot_dir:', chatbot_dir)\n",
    "\n",
    "# Default env for this notebook\n",
    "os.environ.setdefault('LLM_BACKEND', 'hf')\n",
    "os.environ.setdefault('EXTRACTOR_BACKEND', 'hf')\n",
    "if IN_COLAB:\n",
    "    os.environ.setdefault('HF_MODEL', 'Qwen/Qwen2.5-7B-Instruct')\n",
    "else:\n",
    "    os.environ.setdefault('FORCE_CPU', '1')\n",
    "    os.environ.setdefault('HF_MODEL', 'Qwen/Qwen2.5-3B-Instruct')\n",
    "\n",
    "print('HF_MODEL:', os.environ.get('HF_MODEL'))\n",
    "print('FORCE_CPU:', os.environ.get('FORCE_CPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183118cc",
   "metadata": {},
   "source": [
    "# Chạy self-hosted trên RTX 3050 (Windows)\n",
    "\n",
    "Notebook này chạy **self-hosted** bằng HuggingFace (không gọi API LLM bên ngoài).\n",
    "\n",
    "## Về 4-bit (`load_in_4bit=True`)\n",
    "\n",
    "- Trên **Windows native**, `bitsandbytes` 4-bit thường không hoạt động ổn định.\n",
    "\n",
    "- Nếu bạn muốn chạy 7B/14B với 4-bit, khuyến nghị dùng **WSL2 (Ubuntu)** hoặc **Google Colab**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c792e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab setup (chạy trên Colab để không dùng GPU máy local)\n",
    "\n",
    "# Nếu bạn đang chạy local VS Code thì cell này có thể bỏ qua.\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ\n",
    "\n",
    "print('IN_COLAB:', IN_COLAB)\n",
    "\n",
    "\n",
    "\n",
    "# Luôn self-host HF\n",
    "\n",
    "os.environ.setdefault('LLM_BACKEND', 'hf')\n",
    "\n",
    "os.environ.setdefault('EXTRACTOR_BACKEND', 'hf')\n",
    "\n",
    "\n",
    "\n",
    "if IN_COLAB:\n",
    "\n",
    "    # Colab: dùng GPU + có thể 4-bit\n",
    "\n",
    "    os.environ.setdefault('HF_MODEL', 'Qwen/Qwen2.5-7B-Instruct')\n",
    "\n",
    "    # (khuyến nghị) cache vào Drive nếu bạn mount Drive:\n",
    "\n",
    "    # os.environ.setdefault('HF_HOME', '/content/drive/MyDrive/hf_cache')\n",
    "\n",
    "else:\n",
    "\n",
    "    # Local: ép CPU để không tiêu GPU máy bạn\n",
    "\n",
    "    os.environ.setdefault('FORCE_CPU', '1')\n",
    "\n",
    "    os.environ.setdefault('HF_MODEL', 'Qwen/Qwen2.5-3B-Instruct')\n",
    "\n",
    "\n",
    "\n",
    "print('HF_MODEL:', os.environ.get('HF_MODEL'))\n",
    "\n",
    "print('FORCE_CPU:', os.environ.get('FORCE_CPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f21858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra CUDA + VRAM để chọn cấu hình hợp lý\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "print('torch:', torch.__version__)\n",
    "\n",
    "print('cuda available:', torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    print('gpu:', torch.cuda.get_device_name(0))\n",
    "\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "\n",
    "    vram_gb = props.total_memory / (1024**3)\n",
    "\n",
    "    print(f'vram: {vram_gb:.2f} GB')\n",
    "\n",
    "\n",
    "\n",
    "# Gợi ý model mặc định theo VRAM (bạn có thể sửa lại)\n",
    "\n",
    "if os.environ.get('HF_MODEL') is None:\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "\n",
    "        # an toàn cho RTX 3050 phổ biến (4-6GB)\n",
    "\n",
    "        os.environ['HF_MODEL'] = 'Qwen/Qwen2.5-3B-Instruct'\n",
    "\n",
    "    else:\n",
    "\n",
    "        os.environ['HF_MODEL'] = 'Qwen/Qwen2.5-1.5B-Instruct'\n",
    "\n",
    "\n",
    "\n",
    "os.environ.setdefault('LLM_BACKEND', 'hf')\n",
    "\n",
    "os.environ.setdefault('EXTRACTOR_BACKEND', 'hf')\n",
    "\n",
    "print('HF_MODEL:', os.environ['HF_MODEL'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30849616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Colab) Tuỳ chọn: cài thêm nếu muốn chạy 4-bit\n",
    "# - Nếu bạn đã chạy cell \"Auto setup for Colab\" ở đầu notebook thì thường đã cài requirements rồi.\n",
    "# - Cell này chỉ cần khi bạn muốn 4-bit (cần accelerate + bitsandbytes).\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ\n",
    "if not IN_COLAB:\n",
    "    print('Not in Colab; skip')\n",
    "else:\n",
    "    project_root = Path(os.environ.get('PROJECT_ROOT', '/content/23CLCT2_TraditionalMedicineChatbot')).resolve()\n",
    "    req = project_root / 'requirements.txt'\n",
    "    if req.exists():\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-U', 'pip'])\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-U', '-r', str(req)])\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-U', 'accelerate', 'bitsandbytes'])\n",
    "        print('Installed accelerate + bitsandbytes')\n",
    "    else:\n",
    "        raise RuntimeError(f'Không thấy requirements.txt tại: {req}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0764fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Nếu chạy notebook độc lập trên Colab, hãy mount repo hoặc copy folder chatbot/ lên runtime\n",
    "\n",
    "# Ví dụ: from google.colab import drive; drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "\n",
    "# Cấu hình model self-hosting\n",
    "\n",
    "# Gợi ý: trên Windows + RTX 3050, ưu tiên model nhỏ nếu VRAM không nhiều\n",
    "\n",
    "os.environ.setdefault('LLM_BACKEND', 'hf')\n",
    "\n",
    "os.environ.setdefault('EXTRACTOR_BACKEND', 'hf')\n",
    "\n",
    "os.environ.setdefault('HF_MODEL', os.environ.get('HF_MODEL', 'Qwen/Qwen2.5-3B-Instruct'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b028e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thiết lập sys.path để import được modules.*\n",
    "\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "\n",
    "p = repo_root\n",
    "\n",
    "for _ in range(6):\n",
    "\n",
    "    if (p / 'chatbot').exists():\n",
    "\n",
    "        repo_root = p\n",
    "\n",
    "        break\n",
    "\n",
    "    p = p.parent\n",
    "\n",
    "\n",
    "\n",
    "chatbot_dir = (repo_root / 'chatbot').resolve()\n",
    "\n",
    "if str(chatbot_dir) not in sys.path:\n",
    "\n",
    "    sys.path.insert(0, str(chatbot_dir))\n",
    "\n",
    "\n",
    "\n",
    "print('chatbot_dir:', chatbot_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a8f8fa",
   "metadata": {},
   "source": [
    "## Chạy extraction trên Colab và mang JSONL về máy\n",
    "\n",
    "\n",
    "\n",
    "Mục tiêu: chạy LLM **trên Colab GPU**, xuất ra file JSONL (và ảnh nếu bật), sau đó bạn tải JSONL về máy để chạy ingest/webapp bình thường.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6265948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Colab) Mount Drive để lưu output (khuyến nghị)\n",
    "\n",
    "# Nếu không muốn dùng Drive, bạn có thể bỏ qua cell này.\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "\n",
    "    try:\n",
    "\n",
    "        from google.colab import drive  # type: ignore\n",
    "\n",
    "        drive.mount('/content/drive')\n",
    "\n",
    "        print('Drive mounted at /content/drive')\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print('Could not import google.colab.drive:', e)\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Not in Colab; skip drive mount')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chọn đường dẫn project trên Colab\n",
    "\n",
    "# - Nếu bạn clone repo vào /content/23CLCT2_TraditionalMedicineChatbot thì để nguyên.\n",
    "\n",
    "# - Nếu bạn để trong Drive, sửa PROJECT_ROOT cho đúng.\n",
    "\n",
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ\n",
    "\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path(os.environ.get('PROJECT_ROOT', '/content/23CLCT2_TraditionalMedicineChatbot')).resolve()\n",
    "\n",
    "\n",
    "\n",
    "if not PROJECT_ROOT.exists():\n",
    "\n",
    "    print('PROJECT_ROOT does not exist:', PROJECT_ROOT)\n",
    "\n",
    "    if IN_COLAB:\n",
    "\n",
    "        # Tự động clone repo nếu đang chạy trên Colab\n",
    "\n",
    "        default_repo = 'https://github.com/HuyTran28/23CLCT2_TraditionalMedicineChatbot.git'\n",
    "\n",
    "        repo_url = os.environ.get('REPO_URL', default_repo)\n",
    "\n",
    "        print('Cloning from:', repo_url)\n",
    "\n",
    "        import subprocess\n",
    "\n",
    "        subprocess.check_call(['git', 'clone', repo_url, str(PROJECT_ROOT)])\n",
    "\n",
    "        print('Cloned to', PROJECT_ROOT)\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise RuntimeError('Không tìm thấy PROJECT_ROOT. Hãy set env PROJECT_ROOT trỏ đúng thư mục repo.')\n",
    "\n",
    "\n",
    "\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "\n",
    "\n",
    "\n",
    "# Add chatbot/ to sys.path so `from modules...` works\n",
    "\n",
    "chatbot_dir = (PROJECT_ROOT / 'chatbot').resolve()\n",
    "\n",
    "if not chatbot_dir.exists():\n",
    "\n",
    "    raise RuntimeError(f'Không thấy thư mục chatbot/: {chatbot_dir}')\n",
    "\n",
    "\n",
    "\n",
    "if str(chatbot_dir) not in sys.path:\n",
    "\n",
    "    sys.path.insert(0, str(chatbot_dir))\n",
    "\n",
    "\n",
    "\n",
    "print('chatbot_dir:', chatbot_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39967be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Tùy chọn) Nếu chưa có source trên Colab: clone repo\n",
    "\n",
    "# Chạy cell này nếu PROJECT_ROOT chưa tồn tại.\n",
    "\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path(os.environ.get('PROJECT_ROOT', '/content/23CLCT2_TraditionalMedicineChatbot')).resolve()\n",
    "\n",
    "\n",
    "\n",
    "if not PROJECT_ROOT.exists():\n",
    "\n",
    "    # Repo của bạn\n",
    "\n",
    "    default_repo = 'https://github.com/HuyTran28/23CLCT2_TraditionalMedicineChatbot.git'\n",
    "\n",
    "    REPO_URL = os.environ.get('REPO_URL', default_repo)\n",
    "\n",
    "    print('Cloning from:', REPO_URL)\n",
    "\n",
    "    import subprocess\n",
    "\n",
    "    subprocess.check_call(['git', 'clone', REPO_URL, str(PROJECT_ROOT)])\n",
    "\n",
    "    print('Cloned to', PROJECT_ROOT)\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Repo already exists at', PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chạy extraction batch và ghi JSONL\n",
    "from modules.extractor import MedicalDataExtractor\n",
    "from modules.ingest_pipeline import iter_markdown_files, iter_chunks_from_file, extract_chunks_to_jsonl\n",
    "from schemas import medical_schemas\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "assert 'chatbot_dir' in globals(), 'Hãy chạy cell set PROJECT_ROOT trước'\n",
    "\n",
    "# Chọn schema + input\n",
    "SCHEMA_NAME = os.environ.get('SCHEMA_NAME', 'EmergencyProtocol')\n",
    "INPUT_PATH = os.environ.get('INPUT_PATH', str((Path(chatbot_dir) / 'data' / 'raw').resolve()))\n",
    "\n",
    "schema = getattr(medical_schemas, SCHEMA_NAME)\n",
    "print('Schema:', SCHEMA_NAME)\n",
    "print('Input:', INPUT_PATH)\n",
    "\n",
    "# Output JSONL: lưu vào Drive nếu có, không thì lưu /content\n",
    "if IN_COLAB and Path('/content/drive').exists():\n",
    "    out_dir = Path(os.environ.get('OUT_DIR', '/content/drive/MyDrive/tm_outputs')).resolve()\n",
    "else:\n",
    "    out_dir = Path(os.environ.get('OUT_DIR', '/content/tm_outputs')).resolve()\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_JSONL = str(out_dir / f'extracted_{SCHEMA_NAME}.jsonl')\n",
    "print('OUT_JSONL:', OUT_JSONL)\n",
    "\n",
    "# Khởi tạo extractor (tương thích cả code cũ/mới)\n",
    "os.environ.setdefault('EXTRACTOR_BACKEND', 'hf')\n",
    "os.environ.setdefault('LLM_BACKEND', 'hf')\n",
    "\n",
    "try:\n",
    "    # Newer version supports backend=...\n",
    "    extractor = MedicalDataExtractor(backend='hf', model=os.environ['HF_MODEL'], load_in_4bit=None)\n",
    "except TypeError:\n",
    "    # Older version: no backend kwarg; rely on env vars\n",
    "    extractor = MedicalDataExtractor(model=os.environ['HF_MODEL'])\n",
    "\n",
    "# Chạy trên tất cả file .md trong INPUT_PATH\n",
    "total_ok = 0\n",
    "for fp in iter_markdown_files(INPUT_PATH):\n",
    "    chunks = iter_chunks_from_file(fp, schema, chunk_by='book')\n",
    "    n_ok = extract_chunks_to_jsonl(\n",
    "        extractor=extractor,\n",
    "        chunks=chunks,\n",
    "        schema=schema,\n",
    "        out_jsonl_path=OUT_JSONL,\n",
    "        requests_per_minute=float(os.environ.get('RPM', '2')),\n",
    "        enrich_images=(os.environ.get('ENRICH_IMAGES', '0') in {'1','true','yes'}),\n",
    "        resume=True,\n",
    "    )\n",
    "    print('Extracted', n_ok, 'records from', fp)\n",
    "    total_ok += n_ok\n",
    "\n",
    "print('DONE. Total extracted:', total_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Colab) Zip + download JSONL để mang về máy\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "out_jsonl = Path(OUT_JSONL)\n",
    "\n",
    "assert out_jsonl.exists(), f'Không thấy file: {out_jsonl}'\n",
    "\n",
    "\n",
    "\n",
    "zip_path = out_jsonl.with_suffix('.zip')\n",
    "\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', compression=zipfile.ZIP_DEFLATED) as z:\n",
    "\n",
    "    z.write(out_jsonl, arcname=out_jsonl.name)\n",
    "\n",
    "\n",
    "\n",
    "print('Wrote:', zip_path)\n",
    "\n",
    "\n",
    "\n",
    "if IN_COLAB:\n",
    "\n",
    "    try:\n",
    "\n",
    "        from google.colab import files  # type: ignore\n",
    "\n",
    "        files.download(str(zip_path))\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print('Could not import google.colab.files:', e)\n",
    "\n",
    "        print('Zip is at:', zip_path)\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Not in Colab; file is at:', zip_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac395846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Local) Test nhanh 1 đoạn text (tùy chọn)\n",
    "\n",
    "# Cell này chủ yếu để test nhanh khi chạy local; trên Colab bạn có thể bỏ qua.\n",
    "\n",
    "import os\n",
    "\n",
    "from modules.extractor import MedicalDataExtractor\n",
    "\n",
    "from schemas.medical_schemas import MedicinalPlant\n",
    "\n",
    "\n",
    "\n",
    "extractor = MedicalDataExtractor(backend='hf', model=os.environ['HF_MODEL'], load_in_4bit=None)\n",
    "\n",
    "\n",
    "\n",
    "sample_text = '''\n",
    "\n",
    "1. CÂY BÁCH XÙ\n",
    "\n",
    "\n",
    "\n",
    "(Tên khác : Cốt tía)\n",
    "\n",
    "Cây Bách xù thân gỗ nhỏ, cao từ 3 – 4 mét, thân tròn hoặc hơi vuông, cành nhỏ. Ở cành non, lá có hình kim...\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "plant = extractor.extract_single(\n",
    "\n",
    "    text=sample_text,\n",
    "\n",
    "    schema=MedicinalPlant,\n",
    "\n",
    "    context_hint='From chapter on decorative medicinal plants',\n",
    "\n",
    ")\n",
    "\n",
    "plant.model_dump()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
