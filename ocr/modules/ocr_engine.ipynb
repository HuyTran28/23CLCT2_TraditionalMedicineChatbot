{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RCBz_WzSJtyA",
   "metadata": {
    "id": "RCBz_WzSJtyA"
   },
   "outputs": [],
   "source": [
    "# ignore if running outside Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EDSWBTM3Iudm",
   "metadata": {
    "id": "EDSWBTM3Iudm"
   },
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/TraditionalMedicineChatbot/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766fc2e0",
   "metadata": {
    "id": "766fc2e0"
   },
   "source": [
    "# PaddleOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d309a",
   "metadata": {
    "collapsed": true,
    "id": "cc8d309a"
   },
   "outputs": [],
   "source": [
    "!pip install paddlepaddle paddlepaddle-gpu\n",
    "!pip install paddleocr\n",
    "!pip install \"langchain==0.0.353\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac61dff6",
   "metadata": {
    "collapsed": true,
    "id": "ac61dff6"
   },
   "outputs": [],
   "source": [
    "# --- PaddleOCR Initialization ---\n",
    "from paddleocr import PaddleOCR\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='vi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945111e9",
   "metadata": {
    "collapsed": true,
    "id": "945111e9"
   },
   "outputs": [],
   "source": [
    "# --- OCR Usage: Process Image ---\n",
    "import os\n",
    "img_filename = 'sample.png'\n",
    "img_path = os.path.abspath(img_filename) if os.path.exists(img_filename) else None\n",
    "if img_path is None:\n",
    "    raise FileNotFoundError(\"Could not find 'sample.png' locally. Upload it or mount Google Drive and set img_path accordingly.\")\n",
    "print(f'Using image: {img_path}')\n",
    "try:\n",
    "    result = ocr.predict(img_path)\n",
    "except Exception:\n",
    "    result = ocr.ocr(img_path, cls=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ov5thL34eVO2",
   "metadata": {
    "collapsed": true,
    "id": "Ov5thL34eVO2"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 1. Prepare data container\n",
    "output_data = []\n",
    "ocr_data = result[0]\n",
    "\n",
    "rec_texts = ocr_data.get('rec_texts', [])\n",
    "rec_scores = ocr_data.get('rec_scores', [])\n",
    "rec_polys = ocr_data.get('rec_polys', [])\n",
    "\n",
    "# 2. Convert data to standard Python types (to avoid JSON errors)\n",
    "if rec_texts:\n",
    "    for i in range(len(rec_texts)):\n",
    "        # Handle the box: Convert numpy array to list if necessary\n",
    "        box = rec_polys[i]\n",
    "        if isinstance(box, np.ndarray):\n",
    "            box = box.tolist()\n",
    "\n",
    "        # Handle the score: Convert numpy float to python float\n",
    "        score = rec_scores[i]\n",
    "        if isinstance(score, (np.float32, np.float64)):\n",
    "            score = float(score)\n",
    "\n",
    "        # Create a structured dictionary for this detection\n",
    "        detection = {\n",
    "            \"id\": i + 1,\n",
    "            \"text\": rec_texts[i],\n",
    "            \"confidence\": score,\n",
    "            \"box\": box\n",
    "        }\n",
    "        output_data.append(detection)\n",
    "\n",
    "# 3. Write to JSON file\n",
    "output_filename = 'ocr_result.json'\n",
    "try:\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Successfully saved {len(output_data)} detections to '{output_filename}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ImOqbGq-sLR4",
   "metadata": {
    "collapsed": true,
    "id": "ImOqbGq-sLR4"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mb6lzaIXsL8O",
   "metadata": {
    "collapsed": true,
    "id": "mb6lzaIXsL8O"
   },
   "outputs": [],
   "source": [
    "print(\"Loading Text Correction model...\")\n",
    "model_path = \"protonx-models/protonx-legal-tc\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chGTL898sN7y",
   "metadata": {
    "id": "chGTL898sN7y"
   },
   "outputs": [],
   "source": [
    "def correct_text_with_hf(raw_text):\n",
    "    \"\"\"\n",
    "    Takes raw OCR text and passes it through the ProtonX Legal TC model\n",
    "    to fix accents and grammar.\n",
    "    \"\"\"\n",
    "    if not raw_text or len(str(raw_text).strip()) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        raw_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            num_beams=10,\n",
    "            max_new_tokens=128,\n",
    "            length_penalty=1.0,\n",
    "            early_stopping=True,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=2,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0yPTDYVDsQep",
   "metadata": {
    "id": "0yPTDYVDsQep"
   },
   "outputs": [],
   "source": [
    "if 'result' in locals() and result and result[0] is not None:\n",
    "    ocr_data = result[0]\n",
    "\n",
    "    # Extract lists safely\n",
    "    rec_texts = ocr_data.get('rec_texts', [])\n",
    "    rec_scores = ocr_data.get('rec_scores', [])\n",
    "    rec_polys = ocr_data.get('rec_polys', [])\n",
    "\n",
    "    print(f\"Loaded {len(rec_texts)} detected text lines.\")\n",
    "else:\n",
    "    print(\"Variable 'result' is empty or not defined. Please run PaddleOCR first.\")\n",
    "    rec_texts, rec_scores, rec_polys = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1VqiFPnPsYBX",
   "metadata": {
    "collapsed": true,
    "id": "1VqiFPnPsYBX"
   },
   "outputs": [],
   "source": [
    "output_data = []\n",
    "\n",
    "if rec_texts:\n",
    "    total = len(rec_texts)\n",
    "    for i in range(total):\n",
    "        raw_text = rec_texts[i]\n",
    "\n",
    "        # 1. Status Update\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Processing line {i+1}/{total}...\")\n",
    "\n",
    "        # 2. Run Correction\n",
    "        corrected = correct_text_with_hf(raw_text)\n",
    "\n",
    "        # 3. Handle Numpy Types for JSON serialization\n",
    "        # Box\n",
    "        box = rec_polys[i]\n",
    "        if isinstance(box, np.ndarray):\n",
    "            box = box.tolist()\n",
    "\n",
    "        # Score\n",
    "        score = rec_scores[i]\n",
    "        if isinstance(score, (np.float32, np.float64)):\n",
    "            score = float(score)\n",
    "\n",
    "        # 4. Build Dictionary\n",
    "        detection = {\n",
    "            \"id\": i + 1,\n",
    "            \"original_text\": raw_text,\n",
    "            \"corrected_text\": corrected,\n",
    "            \"confidence\": score,\n",
    "            \"box\": box\n",
    "        }\n",
    "        output_data.append(detection)\n",
    "\n",
    "    print(\"Processing complete.\")\n",
    "else:\n",
    "    print(\"No text to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QE0M6X5KsYva",
   "metadata": {
    "id": "QE0M6X5KsYva"
   },
   "outputs": [],
   "source": [
    "output_filename = 'ocr_result_corrected.json'\n",
    "\n",
    "try:\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Successfully saved {len(output_data)} detections to '{output_filename}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSON: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b20b59f",
   "metadata": {
    "id": "8b20b59f"
   },
   "source": [
    "# Enhanced Vietnamese OCR Pipeline\n",
    "\n",
    "This implementation combines:\n",
    "- **CRAFT** for text detection\n",
    "- **VietOCR** with VGG-Transformer for recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78605cb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "78605cb4",
    "outputId": "ed76dea4-782c-4e1f-c5f2-14af55c66e47"
   },
   "outputs": [],
   "source": [
    "%pip install numpy==1.26.4 --no-cache-dir\n",
    "%pip install Pillow==9.5.0\n",
    "%pip install opencv-python==4.7.0.72 opencv-contrib-python==4.7.0.72 --no-cache-dir\n",
    "%pip install torch==1.13.1 torchvision==0.14.1 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "%pip install vietocr==1.3.1\n",
    "%pip install craft-text-detector==0.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22b630",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "9f22b630",
    "outputId": "dba36409-50a4-4cfa-d5f1-ad3747a83b54"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from craft_text_detector import Craft\n",
    "from vietocr.tool.predictor import Predictor\n",
    "from vietocr.tool.config import Cfg\n",
    "\n",
    "print(np.__version__)\n",
    "print(cv2.__version__)\n",
    "print(torch.__version__)\n",
    "\n",
    "# Check device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize CRAFT text detector (best for Vietnamese)\n",
    "print(\"Loading CRAFT detector...\")\n",
    "craft = Craft(output_dir='./craft_output', crop_type=\"poly\", cuda=torch.cuda.is_available())\n",
    "\n",
    "# Initialize VietOCR with best model (VGG-Transformer)\n",
    "print(\"Loading VietOCR recognizer...\")\n",
    "config = Cfg.load_config_from_name('vgg_transformer')\n",
    "config['cnn']['pretrained'] = False\n",
    "config['device'] = device\n",
    "config['predictor']['beamsearch'] = True  # Enable beam search for better accuracy\n",
    "recognizer = Predictor(config)\n",
    "\n",
    "print(\"✓ Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3784243",
   "metadata": {
    "id": "a3784243"
   },
   "outputs": [],
   "source": [
    "# correct imports for modern craft-text-detector\n",
    "from craft_text_detector.image_utils import (\n",
    "    read_image,\n",
    "    normalizeMeanVariance,\n",
    "    resize_aspect_ratio\n",
    ")\n",
    "from craft_text_detector.craft_utils import (\n",
    "    getDetBoxes,\n",
    "    adjustResultCoordinates\n",
    ")\n",
    "\n",
    "\n",
    "def craft_detect_safe(craft, image_path):\n",
    "    # Read image\n",
    "    image = read_image(image_path)\n",
    "\n",
    "    img_resized, target_ratio, size_heatmap = resize_aspect_ratio(\n",
    "        image,\n",
    "        craft.long_size,\n",
    "        interpolation=cv2.INTER_LINEAR\n",
    "    )\n",
    "\n",
    "    ratio_h = ratio_w = 1 / target_ratio\n",
    "\n",
    "    # Step 2 — Normalize image (same as original CRAFT)\n",
    "    x = normalizeMeanVariance(img_resized)\n",
    "    x = torch.from_numpy(x).permute(2, 0, 1).unsqueeze(0).float()\n",
    "\n",
    "    if craft.cuda:\n",
    "        x = x.cuda()\n",
    "\n",
    "    # Step 3 — Get score maps\n",
    "    with torch.no_grad():\n",
    "        y, _ = craft.craft_net(x)\n",
    "\n",
    "    score_text = y[0, :, :, 0].cpu().numpy()\n",
    "    score_link = y[0, :, :, 1].cpu().numpy()\n",
    "\n",
    "    # Step 4 — Detect boxes\n",
    "    boxes, polys = getDetBoxes(\n",
    "        score_text,\n",
    "        score_link,\n",
    "        text_threshold=0.7,  \n",
    "        link_threshold=0.8, \n",
    "        low_text=0.4\n",
    "    )\n",
    "\n",
    "    # Step 5 — Fix polys (drop Nones)\n",
    "    polys = [p for p in polys if p is not None]\n",
    "\n",
    "    # Step 6 — Adjust coordinates\n",
    "    boxes = adjustResultCoordinates(boxes, ratio_w, ratio_h)\n",
    "    polys = adjustResultCoordinates(polys, ratio_w, ratio_h)\n",
    "\n",
    "    return {\n",
    "        \"boxes\": boxes,\n",
    "        \"polys\": polys,\n",
    "        \"heatmap\": score_text\n",
    "    }\n",
    "\n",
    "def group_boxes_to_lines(boxes, y_threshold=10):\n",
    "    \"\"\"\n",
    "    Group word-level boxes into line-level boxes.\n",
    "\n",
    "    Args:\n",
    "        boxes: list of boxes (Nx4x2 arrays)\n",
    "        y_threshold: vertical distance to consider boxes in the same line\n",
    "    Returns:\n",
    "        list of merged boxes per line\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # Compute the center y for each box\n",
    "    box_centers = [np.mean(box[:, 1]) for box in boxes]\n",
    "\n",
    "    # Sort boxes top to bottom\n",
    "    sorted_idx = np.argsort(box_centers)\n",
    "    boxes_sorted = [boxes[i] for i in sorted_idx]\n",
    "    centers_sorted = [box_centers[i] for i in sorted_idx]\n",
    "\n",
    "    lines = []\n",
    "    current_line = [boxes_sorted[0]]\n",
    "    current_y = centers_sorted[0]\n",
    "\n",
    "    for i in range(1, len(boxes_sorted)):\n",
    "        if abs(centers_sorted[i] - current_y) <= y_threshold:\n",
    "            current_line.append(boxes_sorted[i])\n",
    "            current_y = np.mean([current_y, centers_sorted[i]])\n",
    "        else:\n",
    "            lines.append(current_line)\n",
    "            current_line = [boxes_sorted[i]]\n",
    "            current_y = centers_sorted[i]\n",
    "\n",
    "    if current_line:\n",
    "        lines.append(current_line)\n",
    "\n",
    "    # Merge boxes in each line\n",
    "    merged_lines = []\n",
    "    for line in lines:\n",
    "        all_points = np.vstack(line)\n",
    "        x_min = np.min(all_points[:, 0])\n",
    "        y_min = np.min(all_points[:, 1])\n",
    "        x_max = np.max(all_points[:, 0])\n",
    "        y_max = np.max(all_points[:, 1])\n",
    "        merged_lines.append(np.array([[x_min, y_min], [x_max, y_min],\n",
    "                                      [x_max, y_max], [x_min, y_max]]))\n",
    "    return merged_lines\n",
    "\n",
    "\n",
    "def vietnamese_ocr_pipeline(image_path, visualize=True):\n",
    "    \"\"\"\n",
    "    Complete OCR pipeline for Vietnamese text with best-in-class models.\n",
    "\n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        visualize: Whether to show detection visualization\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing detected text and metadata\n",
    "    \"\"\"\n",
    "    # Step 1: Read and preprocess image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Cannot read image: {image_path}\")\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    # Step 2: Detect text regions using CRAFT\n",
    "    prediction_result = craft_detect_safe(craft, image_path)\n",
    "    boxes = prediction_result['boxes']\n",
    "\n",
    "    # Merge small word boxes into line-level boxes\n",
    "    boxes_lines = group_boxes_to_lines(boxes, y_threshold=10)  \n",
    "    regions = boxes_lines\n",
    "\n",
    "    if len(regions) == 0:\n",
    "        print(\"No text detected!\")\n",
    "        return []\n",
    "\n",
    "    # Step 3: Sort regions top-to-bottom, left-to-right (reading order)\n",
    "    def get_region_center(box):\n",
    "        if isinstance(box, dict):\n",
    "            box = box['points']\n",
    "        box = np.array(box)\n",
    "        center_y = np.mean(box[:, 1])\n",
    "        center_x = np.mean(box[:, 0])\n",
    "        return (int(center_y // 30), center_x)  # Group by rows\n",
    "\n",
    "    regions_sorted = sorted(regions, key=get_region_center)\n",
    "\n",
    "    # Step 4: Recognize text in each region\n",
    "    results = []\n",
    "    img_pil = Image.fromarray(img_rgb)\n",
    "\n",
    "    for idx, region in enumerate(regions_sorted):\n",
    "        try:\n",
    "            # Extract bounding box\n",
    "            if isinstance(region, dict):\n",
    "                box = np.array(region['points'], dtype=np.int32)\n",
    "            else:\n",
    "                box = np.array(region, dtype=np.int32)\n",
    "\n",
    "            # Get bounding rectangle with padding\n",
    "            x_min = max(0, np.min(box[:, 0]) - 5)\n",
    "            x_max = min(w, np.max(box[:, 0]) + 5)\n",
    "            y_min = max(0, np.min(box[:, 1]) - 5)\n",
    "            y_max = min(h, np.max(box[:, 1]) + 5)\n",
    "\n",
    "            # Skip if region is too small\n",
    "            if (x_max - x_min) < 10 or (y_max - y_min) < 10:\n",
    "                continue\n",
    "\n",
    "            # Crop region\n",
    "            cropped = img_pil.crop((x_min, y_min, x_max, y_max))\n",
    "\n",
    "            # Recognize text using VietOCR\n",
    "            text = recognizer.predict(cropped)\n",
    "\n",
    "            if text.strip():  # Only include non-empty results\n",
    "                results.append({\n",
    "                    'id': idx + 1,\n",
    "                    'text': text,\n",
    "                    'box': box.tolist(),\n",
    "                    'bbox': [int(x_min), int(y_min), int(x_max), int(y_max)],\n",
    "                    'confidence': 0.95  # CRAFT + VietOCR is highly reliable\n",
    "                })\n",
    "\n",
    "            # Progress update\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"Processed {idx + 1}/{len(regions_sorted)} regions...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing region {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n✓ Successfully extracted {len(results)} text segments\")\n",
    "\n",
    "    # Step 5: Visualization\n",
    "    if visualize and len(results) > 0:\n",
    "        visualize_results(img_rgb, results)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def visualize_results(img_rgb, results):\n",
    "    \"\"\"Visualize OCR results on the image\"\"\"\n",
    "    from PIL import ImageDraw, ImageFont\n",
    "\n",
    "    img_pil = Image.fromarray(img_rgb)\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "\n",
    "    # Try to load a font\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 12)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    for result in results:\n",
    "        box = np.array(result['box'])\n",
    "        text = result['text']\n",
    "        idx = result['id']\n",
    "\n",
    "        # Draw bounding box\n",
    "        points = [tuple(p) for p in box]\n",
    "        draw.polygon(points, outline='#00FF00', width=2)\n",
    "\n",
    "        # Draw text label \n",
    "        label = f\"[{idx}] {text[:30]}...\" if len(text) > 30 else f\"[{idx}] {text}\"\n",
    "        x = int(np.min(box[:, 0]))\n",
    "        y = int(np.min(box[:, 1])) - 20\n",
    "\n",
    "        # Background rectangle\n",
    "        try:\n",
    "            bbox = draw.textbbox((x, y), label, font=font)\n",
    "            draw.rectangle(bbox, fill='white', outline='red')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        draw.text((x, y), label, fill='red', font=font)\n",
    "\n",
    "    # Display\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(img_pil)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Detected {len(results)} text regions')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1859c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "img_filename = 'sample.png'\n",
    "img_path = os.path.abspath(img_filename) if os.path.exists(img_filename) else None\n",
    "if img_path is None:\n",
    "    raise FileNotFoundError(\"Could not find 'sample.png' locally. Upload it or mount Google Drive and set img_path accordingly.\")\n",
    "img_filename = 'sample.png'\n",
    "ocr_results = vietnamese_ocr_pipeline(img_filename, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0141c898",
   "metadata": {
    "id": "0141c898"
   },
   "outputs": [],
   "source": [
    "# Export results to JSON\n",
    "output_filename = 'vietnamese_ocr_results.json'\n",
    "\n",
    "if 'ocr_results' in locals() and ocr_results:\n",
    "    # Prepare data for export\n",
    "    export_data = {\n",
    "        'image': img_filename,\n",
    "        'total_detections': len(ocr_results),\n",
    "        'results': ocr_results\n",
    "    }\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✓ Saved {len(ocr_results)} detections to '{output_filename}'\")\n",
    "\n",
    "    # Also create a plain text version\n",
    "    text_filename = 'vietnamese_ocr_text.txt'\n",
    "    with open(text_filename, 'w', encoding='utf-8') as f:\n",
    "        for result in ocr_results:\n",
    "            text = result.get('text_corrected', result['text'])\n",
    "            f.write(f\"{text}\\n\")\n",
    "\n",
    "    print(f\"✓ Saved plain text to '{text_filename}'\")\n",
    "\n",
    "    # Display summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OCR SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total text segments: {len(ocr_results)}\")\n",
    "    print(f\"\\nExtracted Text:\\n\")\n",
    "    for i, result in enumerate(ocr_results[:10], 1):\n",
    "        text = result.get('text_corrected', result['text'])\n",
    "        print(f\"{i}. {text}\")\n",
    "\n",
    "    if len(ocr_results) > 10:\n",
    "        print(f\"\\n... and {len(ocr_results) - 10} more segments\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"No OCR results to export. Run the pipeline first.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
