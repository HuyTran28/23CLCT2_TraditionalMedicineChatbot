{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "766fc2e0",
      "metadata": {
        "id": "766fc2e0"
      },
      "source": [
        "# PaddleOCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RCBz_WzSJtyA",
      "metadata": {
        "id": "RCBz_WzSJtyA"
      },
      "outputs": [],
      "source": [
        "# ignore if running outside Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EDSWBTM3Iudm",
      "metadata": {
        "id": "EDSWBTM3Iudm"
      },
      "outputs": [],
      "source": [
        "%cd drive/MyDrive/TraditionalMedicineChatbot/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc8d309a",
      "metadata": {
        "collapsed": true,
        "id": "cc8d309a"
      },
      "outputs": [],
      "source": [
        "!pip install paddlepaddle paddlepaddle-gpu\n",
        "!pip install paddleocr\n",
        "!pip install \"langchain==0.0.353\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac61dff6",
      "metadata": {
        "collapsed": true,
        "id": "ac61dff6"
      },
      "outputs": [],
      "source": [
        "# --- PaddleOCR Initialization ---\n",
        "from paddleocr import PaddleOCR\n",
        "ocr = PaddleOCR(use_angle_cls=True, lang='vi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "945111e9",
      "metadata": {
        "collapsed": true,
        "id": "945111e9"
      },
      "outputs": [],
      "source": [
        "# --- OCR Usage: Process Image ---\n",
        "import os\n",
        "img_filename = 'sample.png'\n",
        "img_path = os.path.abspath(img_filename) if os.path.exists(img_filename) else None\n",
        "if img_path is None:\n",
        "    raise FileNotFoundError(\"Could not find 'sample.png' locally. Upload it or mount Google Drive and set img_path accordingly.\")\n",
        "print(f'Using image: {img_path}')\n",
        "try:\n",
        "    result = ocr.predict(img_path)\n",
        "except Exception:\n",
        "    result = ocr.ocr(img_path, cls=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ov5thL34eVO2",
      "metadata": {
        "collapsed": true,
        "id": "Ov5thL34eVO2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# 1. Prepare data container\n",
        "output_data = []\n",
        "ocr_data = result[0]\n",
        "\n",
        "rec_texts = ocr_data.get('rec_texts', [])\n",
        "rec_scores = ocr_data.get('rec_scores', [])\n",
        "rec_polys = ocr_data.get('rec_polys', [])\n",
        "\n",
        "# 2. Convert data to standard Python types (to avoid JSON errors)\n",
        "if rec_texts:\n",
        "    for i in range(len(rec_texts)):\n",
        "        # Handle the box: Convert numpy array to list if necessary\n",
        "        box = rec_polys[i]\n",
        "        if isinstance(box, np.ndarray):\n",
        "            box = box.tolist()\n",
        "\n",
        "        # Handle the score: Convert numpy float to python float\n",
        "        score = rec_scores[i]\n",
        "        if isinstance(score, (np.float32, np.float64)):\n",
        "            score = float(score)\n",
        "\n",
        "        # Create a structured dictionary for this detection\n",
        "        detection = {\n",
        "            \"id\": i + 1,\n",
        "            \"text\": rec_texts[i],\n",
        "            \"confidence\": score,\n",
        "            \"box\": box\n",
        "        }\n",
        "        output_data.append(detection)\n",
        "\n",
        "# 3. Write to JSON file\n",
        "output_filename = 'ocr_result.json'\n",
        "try:\n",
        "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Successfully saved {len(output_data)} detections to '{output_filename}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving JSON: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ImOqbGq-sLR4",
      "metadata": {
        "collapsed": true,
        "id": "ImOqbGq-sLR4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mb6lzaIXsL8O",
      "metadata": {
        "collapsed": true,
        "id": "mb6lzaIXsL8O"
      },
      "outputs": [],
      "source": [
        "print(\"Loading Text Correction model...\")\n",
        "model_path = \"protonx-models/protonx-legal-tc\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(\"Model loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "chGTL898sN7y",
      "metadata": {
        "id": "chGTL898sN7y"
      },
      "outputs": [],
      "source": [
        "def correct_text_with_hf(raw_text):\n",
        "    \"\"\"\n",
        "    Takes raw OCR text and passes it through the ProtonX Legal TC model\n",
        "    to fix accents and grammar.\n",
        "    \"\"\"\n",
        "    if not raw_text or len(str(raw_text).strip()) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        raw_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            num_beams=10,\n",
        "            max_new_tokens=128,\n",
        "            length_penalty=1.0,\n",
        "            early_stopping=True,\n",
        "            repetition_penalty=1.2,\n",
        "            no_repeat_ngram_size=2,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0yPTDYVDsQep",
      "metadata": {
        "id": "0yPTDYVDsQep"
      },
      "outputs": [],
      "source": [
        "if 'result' in locals() and result and result[0] is not None:\n",
        "    ocr_data = result[0]\n",
        "\n",
        "    # Extract lists safely\n",
        "    rec_texts = ocr_data.get('rec_texts', [])\n",
        "    rec_scores = ocr_data.get('rec_scores', [])\n",
        "    rec_polys = ocr_data.get('rec_polys', [])\n",
        "\n",
        "    print(f\"Loaded {len(rec_texts)} detected text lines.\")\n",
        "else:\n",
        "    print(\"Variable 'result' is empty or not defined. Please run PaddleOCR first.\")\n",
        "    rec_texts, rec_scores, rec_polys = [], [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1VqiFPnPsYBX",
      "metadata": {
        "collapsed": true,
        "id": "1VqiFPnPsYBX"
      },
      "outputs": [],
      "source": [
        "output_data = []\n",
        "\n",
        "if rec_texts:\n",
        "    total = len(rec_texts)\n",
        "    for i in range(total):\n",
        "        raw_text = rec_texts[i]\n",
        "\n",
        "        # 1. Status Update\n",
        "        if i % 5 == 0:\n",
        "            print(f\"Processing line {i+1}/{total}...\")\n",
        "\n",
        "        # 2. Run Correction\n",
        "        corrected = correct_text_with_hf(raw_text)\n",
        "\n",
        "        # 3. Handle Numpy Types for JSON serialization\n",
        "        # Box\n",
        "        box = rec_polys[i]\n",
        "        if isinstance(box, np.ndarray):\n",
        "            box = box.tolist()\n",
        "\n",
        "        # Score\n",
        "        score = rec_scores[i]\n",
        "        if isinstance(score, (np.float32, np.float64)):\n",
        "            score = float(score)\n",
        "\n",
        "        # 4. Build Dictionary\n",
        "        detection = {\n",
        "            \"id\": i + 1,\n",
        "            \"original_text\": raw_text,\n",
        "            \"corrected_text\": corrected,\n",
        "            \"confidence\": score,\n",
        "            \"box\": box\n",
        "        }\n",
        "        output_data.append(detection)\n",
        "\n",
        "    print(\"Processing complete.\")\n",
        "else:\n",
        "    print(\"No text to process.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QE0M6X5KsYva",
      "metadata": {
        "id": "QE0M6X5KsYva"
      },
      "outputs": [],
      "source": [
        "output_filename = 'ocr_result_corrected.json'\n",
        "\n",
        "try:\n",
        "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Successfully saved {len(output_data)} detections to '{output_filename}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving JSON: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o9gSu-esuFF2",
      "metadata": {
        "id": "o9gSu-esuFF2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jzFUQ_9dufSx",
      "metadata": {
        "id": "jzFUQ_9dufSx"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('Roboto-Regular.ttf'):\n",
        "    !wget -q -O Roboto-Regular.ttf https://github.com/googlefonts/roboto/raw/main/src/hinted/Roboto-Regular.ttf\n",
        "    print(\"Font downloaded.\")\n",
        "else:\n",
        "    print(\"Font already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c90IjFHpu2Ze",
      "metadata": {
        "id": "c90IjFHpu2Ze"
      },
      "outputs": [],
      "source": [
        "def visualize_ocr_in_colab(image_path, ocr_results):\n",
        "  if not os.path.exists(image_path):\n",
        "      print(f\"ERROR: Image not found at {image_path}\")\n",
        "      print(\"Tip: Drag and drop your image into the 'Files' folder on the left sidebar.\")\n",
        "      return\n",
        "\n",
        "  # Load Font\n",
        "  try:\n",
        "      font = ImageFont.truetype(\"Roboto-Regular.ttf\", 8)\n",
        "  except:\n",
        "      font = ImageFont.load_default()\n",
        "\n",
        "  # Load Image\n",
        "  img_cv2 = cv2.imread(image_path)\n",
        "  if img_cv2 is None:\n",
        "      print(\"Could not read image. Check file format.\")\n",
        "      return\n",
        "\n",
        "  img_rgb = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "  pil_img = Image.fromarray(img_rgb)\n",
        "  draw = ImageDraw.Draw(pil_img)\n",
        "\n",
        "  print(f\"Visualizing {len(ocr_results)} detections...\")\n",
        "\n",
        "  for item in ocr_results:\n",
        "      # Extract data\n",
        "      box = np.array(item['box'], dtype=np.int32)\n",
        "      text_corrected = item['corrected_text']\n",
        "      id_num = item['id']\n",
        "\n",
        "      # 1. Draw Box (Green)\n",
        "      # Convert numpy box to list of tuples for PIL\n",
        "      flat_box = [tuple(point) for point in box]\n",
        "      draw.polygon(flat_box, outline=\"#00FF00\", width=3)\n",
        "\n",
        "      # 2. Draw Text (Red on White BG)\n",
        "      label = f\"[{id_num}] {text_corrected}\"\n",
        "\n",
        "      # Position: Top-left of the box\n",
        "      txt_x = np.min(box[:, 0])\n",
        "      txt_y = np.min(box[:, 1]) - 30 # Move up a bit\n",
        "\n",
        "      # Draw background rectangle for text (better visibility)\n",
        "      try:\n",
        "          left, top, right, bottom = draw.textbbox((txt_x, txt_y), label, font=font)\n",
        "          draw.rectangle((left-5, top-5, right+5, bottom+5), fill=\"white\")\n",
        "      except:\n",
        "          pass # fallback for older PIL versions\n",
        "\n",
        "      draw.text((txt_x, txt_y), label, font=font, fill=\"#FF0000\")\n",
        "\n",
        "  # Display using Matplotlib (Safe for Colab)\n",
        "  plt.figure(figsize=(20, 20))\n",
        "  plt.imshow(np.array(pil_img))\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "  print(\"Visualization function ready.\")\n",
        "\n",
        "\n",
        "# Ensure output_data exists from your previous cells before running this\n",
        "if 'output_data' in locals() and os.path.exists(img_path):\n",
        "    visualize_ocr_in_colab(img_path, output_data)\n",
        "else:\n",
        "    print(\"Cannot visualize: Make sure 'output_data' exists and IMAGE_PATH is correct.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KeXI8XiKnmnv",
      "metadata": {
        "collapsed": true,
        "id": "KeXI8XiKnmnv"
      },
      "outputs": [],
      "source": [
        "# 1. Install\n",
        "! pip install --quiet vietocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RZ2T2VTmpB7n",
      "metadata": {
        "id": "RZ2T2VTmpB7n"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from vietocr.tool.predictor import Predictor\n",
        "from vietocr.tool.config import Cfg\n",
        "\n",
        "# --- Setup VietOCR ---\n",
        "config = Cfg.load_config_from_name('vgg_transformer')\n",
        "config['cnn']['pretrained'] = False\n",
        "config['device'] = 'cuda:0' # Use 'cpu' if no GPU\n",
        "config['predictor']['beamsearch'] = False\n",
        "recognizer = Predictor(config)\n",
        "\n",
        "def debug_and_read(image_path):\n",
        "    # 1. Load and Resize\n",
        "    # Resizing to a fixed width helps the \"dilation\" kernel work consistently\n",
        "    img_cv = cv2.imread(image_path)\n",
        "    if img_cv is None:\n",
        "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "\n",
        "    target_width = 1500\n",
        "    h, w = img_cv.shape[:2]\n",
        "    scale = target_width / w\n",
        "    new_h = int(h * scale)\n",
        "    img_cv = cv2.resize(img_cv, (target_width, new_h))\n",
        "\n",
        "    # 2. Convert to Black/White for detection\n",
        "    gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Use adaptive thresholding to handle shadows/lighting better\n",
        "    binary = cv2.adaptiveThreshold(\n",
        "        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "        cv2.THRESH_BINARY_INV, 21, 10\n",
        "    )\n",
        "\n",
        "    # 3. Dilate to connect words into lines\n",
        "    # Kernel size: (Wide, Short). Wide to connect words horizontally.\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25, 3))\n",
        "    dilated = cv2.dilate(binary, kernel, iterations=2)\n",
        "\n",
        "    # 4. Find Contours\n",
        "    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Sort from Top to Bottom\n",
        "    bounding_boxes = [cv2.boundingRect(c) for c in contours]\n",
        "    bounding_boxes.sort(key=lambda x: x[1])\n",
        "\n",
        "    # 5. Draw Boxes & Read Text\n",
        "    output_image = img_cv.copy()\n",
        "    full_text = []\n",
        "\n",
        "    print(f\"Detected {len(bounding_boxes)} potential lines...\")\n",
        "\n",
        "    for x, y, w, h in bounding_boxes:\n",
        "        # Filter noise: Box must be reasonable size\n",
        "        if h < 10 or w < 20:\n",
        "            continue\n",
        "        # Filter \"Whole Page\" borders: Ignore if box is > 90% of image area\n",
        "        if (w * h) > (0.9 * img_cv.shape[0] * img_cv.shape[1]):\n",
        "            continue\n",
        "\n",
        "        # Draw red box for debugging\n",
        "        cv2.rectangle(output_image, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
        "\n",
        "        # Crop and Read\n",
        "        # Add padding\n",
        "        pad = 5\n",
        "        crop = img_cv[max(0, y-pad):min(new_h, y+h+pad), max(0, x-pad):min(target_width, x+w+pad)]\n",
        "\n",
        "        # Convert to PIL for VietOCR\n",
        "        crop_pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        try:\n",
        "            text = recognizer.predict(crop_pil)\n",
        "            full_text.append(text)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # 6. Show the Debug Image\n",
        "    plt.figure(figsize=(10, 15))\n",
        "    plt.imshow(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Red boxes = Detected Lines\")\n",
        "    plt.show()\n",
        "\n",
        "    return \"\\n\".join(full_text)\n",
        "\n",
        "# Run it\n",
        "result = debug_and_read('sample.png')\n",
        "print(\"\\n--- Extracted Text ---\\n\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b20b59f",
      "metadata": {
        "id": "8b20b59f"
      },
      "source": [
        "# Enhanced Vietnamese OCR Pipeline\n",
        "**Best-in-class models for Vietnamese text extraction (2025)**\n",
        "\n",
        "This implementation combines:\n",
        "- **CRAFT** for text detection (State-of-the-art detector)\n",
        "- **VietOCR** with VGG-Transformer for recognition\n",
        "- **PhoBERT** for post-processing text correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78605cb4",
      "metadata": {
        "collapsed": true,
        "id": "78605cb4"
      },
      "outputs": [],
      "source": [
        "# Install required packages for enhanced OCR\n",
        "!pip install -q opencv-python-headless\n",
        "!pip install vietocr\n",
        "!pip install pillow==10.2.0\n",
        "!pip install craft-text-detector\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q transformers\n",
        "!pip install -q matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f22b630",
      "metadata": {
        "id": "9f22b630"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from vietocr.tool.predictor import Predictor\n",
        "from vietocr.tool.config import Cfg\n",
        "from craft_text_detector import Craft\n",
        "import json\n",
        "\n",
        "# Check device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize CRAFT text detector (best for Vietnamese)\n",
        "print(\"Loading CRAFT detector...\")\n",
        "craft = Craft(output_dir='./craft_output', crop_type=\"poly\", cuda=torch.cuda.is_available())\n",
        "\n",
        "# Initialize VietOCR with best model (VGG-Transformer)\n",
        "print(\"Loading VietOCR recognizer...\")\n",
        "config = Cfg.load_config_from_name('vgg_transformer')\n",
        "config['cnn']['pretrained'] = False\n",
        "config['device'] = device\n",
        "config['predictor']['beamsearch'] = True  # Enable beam search for better accuracy\n",
        "recognizer = Predictor(config)\n",
        "\n",
        "print(\"âœ“ Models loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3784243",
      "metadata": {
        "id": "a3784243"
      },
      "outputs": [],
      "source": [
        "def vietnamese_ocr_pipeline(image_path, visualize=True):\n",
        "    \"\"\"\n",
        "    Complete OCR pipeline for Vietnamese text with best-in-class models.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to the image file\n",
        "        visualize: Whether to show detection visualization\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries containing detected text and metadata\n",
        "    \"\"\"\n",
        "    print(f\"Processing: {image_path}\")\n",
        "\n",
        "    # Step 1: Read and preprocess image\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        raise FileNotFoundError(f\"Cannot read image: {image_path}\")\n",
        "\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    h, w = img.shape[:2]\n",
        "    print(f\"Image size: {w}x{h}\")\n",
        "\n",
        "    # Step 2: Detect text regions using CRAFT\n",
        "    print(\"Detecting text regions...\")\n",
        "    prediction_result = craft.detect_text(image_path)\n",
        "\n",
        "    # Extract regions\n",
        "    regions = prediction_result.get('boxes', [])\n",
        "    print(f\"Found {len(regions)} text regions\")\n",
        "\n",
        "    if len(regions) == 0:\n",
        "        print(\"No text detected!\")\n",
        "        return []\n",
        "\n",
        "    # Step 3: Sort regions top-to-bottom, left-to-right (reading order)\n",
        "    def get_region_center(box):\n",
        "        if isinstance(box, dict):\n",
        "            box = box['points']\n",
        "        box = np.array(box)\n",
        "        center_y = np.mean(box[:, 1])\n",
        "        center_x = np.mean(box[:, 0])\n",
        "        return (int(center_y // 30), center_x)  # Group by rows\n",
        "\n",
        "    regions_sorted = sorted(regions, key=get_region_center)\n",
        "\n",
        "    # Step 4: Recognize text in each region\n",
        "    results = []\n",
        "    img_pil = Image.fromarray(img_rgb)\n",
        "\n",
        "    for idx, region in enumerate(regions_sorted):\n",
        "        try:\n",
        "            # Extract bounding box\n",
        "            if isinstance(region, dict):\n",
        "                box = np.array(region['points'], dtype=np.int32)\n",
        "            else:\n",
        "                box = np.array(region, dtype=np.int32)\n",
        "\n",
        "            # Get bounding rectangle with padding\n",
        "            x_min = max(0, np.min(box[:, 0]) - 5)\n",
        "            x_max = min(w, np.max(box[:, 0]) + 5)\n",
        "            y_min = max(0, np.min(box[:, 1]) - 5)\n",
        "            y_max = min(h, np.max(box[:, 1]) + 5)\n",
        "\n",
        "            # Skip if region is too small\n",
        "            if (x_max - x_min) < 10 or (y_max - y_min) < 10:\n",
        "                continue\n",
        "\n",
        "            # Crop region\n",
        "            cropped = img_pil.crop((x_min, y_min, x_max, y_max))\n",
        "\n",
        "            # Recognize text using VietOCR\n",
        "            text = recognizer.predict(cropped)\n",
        "\n",
        "            if text.strip():  # Only include non-empty results\n",
        "                results.append({\n",
        "                    'id': idx + 1,\n",
        "                    'text': text,\n",
        "                    'box': box.tolist(),\n",
        "                    'bbox': [int(x_min), int(y_min), int(x_max), int(y_max)],\n",
        "                    'confidence': 0.95  # CRAFT + VietOCR is highly reliable\n",
        "                })\n",
        "\n",
        "            # Progress update\n",
        "            if (idx + 1) % 10 == 0:\n",
        "                print(f\"Processed {idx + 1}/{len(regions_sorted)} regions...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing region {idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\nâœ“ Successfully extracted {len(results)} text segments\")\n",
        "\n",
        "    # Step 5: Visualization (optional)\n",
        "    if visualize and len(results) > 0:\n",
        "        visualize_results(img_rgb, results)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def visualize_results(img_rgb, results):\n",
        "    \"\"\"Visualize OCR results on the image\"\"\"\n",
        "    from PIL import ImageDraw, ImageFont\n",
        "\n",
        "    img_pil = Image.fromarray(img_rgb)\n",
        "    draw = ImageDraw.Draw(img_pil)\n",
        "\n",
        "    # Try to load a font\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 12)\n",
        "    except:\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    for result in results:\n",
        "        box = np.array(result['box'])\n",
        "        text = result['text']\n",
        "        idx = result['id']\n",
        "\n",
        "        # Draw bounding box (green)\n",
        "        points = [tuple(p) for p in box]\n",
        "        draw.polygon(points, outline='#00FF00', width=2)\n",
        "\n",
        "        # Draw text label (red on white background)\n",
        "        label = f\"[{idx}] {text[:30]}...\" if len(text) > 30 else f\"[{idx}] {text}\"\n",
        "        x = int(np.min(box[:, 0]))\n",
        "        y = int(np.min(box[:, 1])) - 20\n",
        "\n",
        "        # Background rectangle\n",
        "        try:\n",
        "            bbox = draw.textbbox((x, y), label, font=font)\n",
        "            draw.rectangle(bbox, fill='white', outline='red')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        draw.text((x, y), label, fill='red', font=font)\n",
        "\n",
        "    # Display\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    plt.imshow(img_pil)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Detected {len(results)} text regions')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage - set your image path\n",
        "img_filename = 'sample.png'\n",
        "ocr_results = vietnamese_ocr_pipeline(img_filename, visualize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d703fae",
      "metadata": {
        "id": "0d703fae"
      },
      "outputs": [],
      "source": [
        "# Optional: Install text correction model for post-processing\n",
        "!pip install -q underthesea\n",
        "!pip install -q py_vncorenlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c47b0630",
      "metadata": {
        "id": "c47b0630"
      },
      "outputs": [],
      "source": [
        "from underthesea import word_tokenize\n",
        "import re\n",
        "\n",
        "def post_process_vietnamese_text(text):\n",
        "    \"\"\"\n",
        "    Post-process OCR text to fix common Vietnamese OCR errors.\n",
        "\n",
        "    Args:\n",
        "        text: Raw OCR output text\n",
        "\n",
        "    Returns:\n",
        "        Corrected text\n",
        "    \"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return \"\"\n",
        "\n",
        "    # Fix common OCR mistakes for Vietnamese\n",
        "    corrections = {\n",
        "        # Common character confusions\n",
        "        '0': 'O',  # In words, 0 is usually O\n",
        "        'l': 'I',  # In uppercase contexts\n",
        "        '|': 'I',\n",
        "        '1': 'l',  # In lowercase contexts\n",
        "\n",
        "        # Vietnamese specific\n",
        "        'Ä‘': 'Ä‘',  # Normalize Ä‘ character\n",
        "        'Ä': 'Ä',\n",
        "\n",
        "        # Remove weird spacing\n",
        "        ' ,': ',',\n",
        "        ' .': '.',\n",
        "        ' :': ':',\n",
        "        ' ;': ';',\n",
        "        '( ': '(',\n",
        "        ' )': ')',\n",
        "    }\n",
        "\n",
        "    corrected = text\n",
        "    for wrong, right in corrections.items():\n",
        "        corrected = corrected.replace(wrong, right)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    corrected = re.sub(r'\\s+', ' ', corrected).strip()\n",
        "\n",
        "    # Basic Vietnamese word tokenization for validation\n",
        "    try:\n",
        "        tokens = word_tokenize(corrected, format=\"text\")\n",
        "        return tokens\n",
        "    except:\n",
        "        return corrected\n",
        "\n",
        "\n",
        "# Apply post-processing to OCR results\n",
        "if 'ocr_results' in locals() and ocr_results:\n",
        "    print(\"Applying post-processing...\")\n",
        "    for result in ocr_results:\n",
        "        original = result['text']\n",
        "        corrected = post_process_vietnamese_text(original)\n",
        "        result['text_corrected'] = corrected\n",
        "\n",
        "        if original != corrected:\n",
        "            print(f\"Corrected: '{original}' â†’ '{corrected}'\")\n",
        "\n",
        "    print(f\"\\nâœ“ Post-processing complete for {len(ocr_results)} segments\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0141c898",
      "metadata": {
        "id": "0141c898"
      },
      "outputs": [],
      "source": [
        "# Export results to JSON\n",
        "output_filename = 'vietnamese_ocr_results.json'\n",
        "\n",
        "if 'ocr_results' in locals() and ocr_results:\n",
        "    # Prepare data for export\n",
        "    export_data = {\n",
        "        'image': img_filename,\n",
        "        'total_detections': len(ocr_results),\n",
        "        'results': ocr_results\n",
        "    }\n",
        "\n",
        "    # Save to JSON\n",
        "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"âœ“ Saved {len(ocr_results)} detections to '{output_filename}'\")\n",
        "\n",
        "    # Also create a plain text version\n",
        "    text_filename = 'vietnamese_ocr_text.txt'\n",
        "    with open(text_filename, 'w', encoding='utf-8') as f:\n",
        "        for result in ocr_results:\n",
        "            text = result.get('text_corrected', result['text'])\n",
        "            f.write(f\"{text}\\n\")\n",
        "\n",
        "    print(f\"âœ“ Saved plain text to '{text_filename}'\")\n",
        "\n",
        "    # Display summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"OCR SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total text segments: {len(ocr_results)}\")\n",
        "    print(f\"\\nExtracted Text:\\n\")\n",
        "    for i, result in enumerate(ocr_results[:10], 1):  # Show first 10\n",
        "        text = result.get('text_corrected', result['text'])\n",
        "        print(f\"{i}. {text}\")\n",
        "\n",
        "    if len(ocr_results) > 10:\n",
        "        print(f\"\\n... and {len(ocr_results) - 10} more segments\")\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"No OCR results to export. Run the pipeline first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2329955",
      "metadata": {
        "id": "f2329955"
      },
      "source": [
        "## ðŸ“‹ Usage Instructions\n",
        "\n",
        "**To use this enhanced Vietnamese OCR pipeline:**\n",
        "\n",
        "1. **Install dependencies** - Run the installation cell above\n",
        "2. **Initialize models** - Load CRAFT detector and VietOCR recognizer\n",
        "3. **Process image** - Call `vietnamese_ocr_pipeline('your_image.png')`\n",
        "4. **Post-process** (optional) - Apply text corrections\n",
        "5. **Export results** - Save to JSON and text files\n",
        "\n",
        "**Why this is the best approach for Vietnamese:**\n",
        "- âœ… **CRAFT**: State-of-the-art text detection (works on any script)\n",
        "- âœ… **VietOCR**: Specifically trained on Vietnamese text with transformer architecture\n",
        "- âœ… **Underthesea**: Vietnamese NLP toolkit for post-processing\n",
        "- âœ… **High accuracy**: Combines best detector + best Vietnamese recognizer\n",
        "\n",
        "**Performance tips:**\n",
        "- For scanned documents: Use high-resolution images (300+ DPI)\n",
        "- For photos: Ensure good lighting and minimal skew\n",
        "- GPU recommended for faster processing"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}