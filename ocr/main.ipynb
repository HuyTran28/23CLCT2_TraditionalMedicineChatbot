{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e9e5d63e",
      "metadata": {
        "id": "e9e5d63e"
      },
      "source": [
        "## 1. Setup: Clone Repository & Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d4d0063",
      "metadata": {
        "id": "5d4d0063"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Create working directory\n",
        "WORK_DIR = \"/content/traditional_medicine_ocr\"\n",
        "Path(WORK_DIR).mkdir(exist_ok=True)\n",
        "os.chdir(WORK_DIR)\n",
        "\n",
        "print(f\"Working directory: {WORK_DIR}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLONING REPOSITORY...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/HuyTran28/23CLCT2_TraditionalMedicineChatbot.git repo\n",
        "\n",
        "print(\"\\nRepository cloned successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b3c038c",
      "metadata": {
        "id": "9b3c038c"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INSTALLING DEPENDENCIES (GPU OPTIMIZED)...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Install pip packages with GPU support\n",
        "!pip install --upgrade pip setuptools wheel -q\n",
        "\n",
        "# Install core dependencies\n",
        "!pip install -q numpy>=1.26.4\n",
        "!pip install -q Pillow>=10.1.0,<11.0.0\n",
        "!pip install -q matplotlib>=3.8.0\n",
        "!pip install -q PyMuPDF>=1.23.8\n",
        "!pip install -q python-docx>=1.1.0\n",
        "!pip install -q pdf2docx>=0.5.8\n",
        "\n",
        "# Install OCR and NLP dependencies\n",
        "!pip install -q underthesea>=6.0.0\n",
        "!pip install -q pyvi>=0.1.1\n",
        "!pip install -q opencv-contrib-python>=4.8.0.74\n",
        "!pip install -q opencv-python>=4.8.0.74\n",
        "\n",
        "# Install marker-pdf (GPU enabled on Colab)\n",
        "print(\"\\nInstalling marker-pdf (this may take a few minutes)...\")\n",
        "!pip install -q marker-pdf>=0.2.0\n",
        "\n",
        "# Verify GPU availability for marker-pdf\n",
        "print(\"\\nVerifying GPU availability...\")\n",
        "!nvidia-smi --query-gpu=name --format=csv,noheader\n",
        "\n",
        "print(\"\\nAll dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e115c49a",
      "metadata": {
        "id": "e115c49a"
      },
      "source": [
        "## 2. Setup Environment & Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd9eba44",
      "metadata": {
        "id": "dd9eba44"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Optional, List, Dict, Any\n",
        "import shutil\n",
        "import json\n",
        "from datetime import datetime\n",
        "import concurrent.futures\n",
        "from multiprocessing import cpu_count\n",
        "import zipfile\n",
        "import time\n",
        "\n",
        "# Add OCR modules to path\n",
        "ocr_module_path = Path(WORK_DIR) / \"repo\" / \"ocr\"\n",
        "sys.path.insert(0, str(ocr_module_path))\n",
        "sys.path.insert(0, str(ocr_module_path.parent))\n",
        "\n",
        "# Import pipeline and modules\n",
        "from modules.pipeline import OCRPipeline\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Create directories\n",
        "INPUT_DIR = Path(WORK_DIR) / \"input\"\n",
        "OUTPUT_DIR = Path(WORK_DIR) / \"output\"\n",
        "TEMP_DIR = Path(WORK_DIR) / \"temp\"\n",
        "\n",
        "INPUT_DIR.mkdir(exist_ok=True)\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "TEMP_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Environment setup complete\")\n",
        "print(f\"Input directory: {INPUT_DIR}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"CPU Count: {cpu_count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0ff6ac6",
      "metadata": {
        "id": "b0ff6ac6"
      },
      "source": [
        "## 3. Upload Input Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0289d9eb",
      "metadata": {
        "id": "0289d9eb"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"UPLOAD PDF FILES\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nClick 'Choose Files' to select PDF(s) from your computer.\")\n",
        "print(\"You can upload multiple PDFs at once.\")\n",
        "print(\"\\nSupported: Single or batch processing of PDFs (including ~300+ pages)\\n\")\n",
        "\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# Move uploaded files to input directory\n",
        "uploaded_count = 0\n",
        "for filename, data in uploaded_files.items():\n",
        "    if filename.lower().endswith('.pdf'):\n",
        "        input_path = INPUT_DIR / filename\n",
        "        with open(input_path, 'wb') as f:\n",
        "            f.write(data)\n",
        "        file_size_mb = input_path.stat().st_size / (1024*1024)\n",
        "        print(f\"{filename} ({file_size_mb:.2f} MB) uploaded successfully\")\n",
        "        uploaded_count += 1\n",
        "    else:\n",
        "        print(f\"Skipped {filename} (not a PDF)\")\n",
        "\n",
        "print(f\"\\nTotal PDFs uploaded: {uploaded_count}\")\n",
        "\n",
        "# List uploaded files\n",
        "pdf_files = sorted(INPUT_DIR.glob(\"*.pdf\"))\n",
        "print(f\"\\nPDF files ready for processing:\")\n",
        "for pdf in pdf_files:\n",
        "    print(f\"  - {pdf.name} ({pdf.stat().st_size / (1024*1024):.2f} MB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3648187",
      "metadata": {
        "id": "b3648187"
      },
      "source": [
        "## 4. Configure Processing Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28f17173",
      "metadata": {
        "id": "28f17173"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PROCESSING CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "# Number of parallel workers for processing\n",
        "# NUM_WORKERS = cpu_count()\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "PROCESSING_MODE = \"scan\"\n",
        "\n",
        "# PDF conversion settings\n",
        "DPI = 300  # Resolution for PDF to image conversion\n",
        "EXTRACT_IMAGES = True  # Extract images from PDF\n",
        "PRESERVE_LAYOUT = True  # Preserve document layout/structure\n",
        "EXTRACT_TABLES = True  # Extract and process tables\n",
        "ENABLE_PREPROCESSING = True  # Preprocess scanned pages\n",
        "\n",
        "# LLM correction\n",
        "USE_LLM_CORRECTION = True\n",
        "\n",
        "# Display configuration\n",
        "print(\"=\"*60)\n",
        "print(\"PROCESSING CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Number of Workers (CPU cores): {NUM_WORKERS}\")\n",
        "print(f\"Processing Mode: {PROCESSING_MODE}\")\n",
        "print(f\"DPI: {DPI}\")\n",
        "print(f\"Extract Images: {EXTRACT_IMAGES}\")\n",
        "print(f\"Preserve Layout: {PRESERVE_LAYOUT}\")\n",
        "print(f\"Extract Tables: {EXTRACT_TABLES}\")\n",
        "print(f\"Enable Preprocessing: {ENABLE_PREPROCESSING}\")\n",
        "print(f\"LLM Correction: {USE_LLM_CORRECTION}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bf1ad44",
      "metadata": {
        "id": "7bf1ad44"
      },
      "source": [
        "## 5. Initialize OCR Pipeline with Parallelism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf74a588",
      "metadata": {
        "id": "bf74a588"
      },
      "outputs": [],
      "source": [
        "print(\"\\nInitializing OCR Pipeline...\\n\")\n",
        "\n",
        "import os\n",
        "os.environ[\"TQDM_MININTERVAL\"] = os.environ.get(\"TQDM_MININTERVAL\", \"1.0\")  # seconds between refreshes\n",
        "os.environ[\"TQDM_MAX_INTERVAL\"] = os.environ.get(\"TQDM_MAX_INTERVAL\", \"5.0\")  # cap refresh interval\n",
        "\n",
        "class SimpleProgress:\n",
        "    def __init__(self, iterable=None, total=None, desc=None):\n",
        "        self.iterable = iterable if iterable is not None else []\n",
        "        self.total = total if total is not None else (len(self.iterable) if hasattr(self.iterable, \"__len__\") else None)\n",
        "        self.desc = desc\n",
        "    def __iter__(self):\n",
        "        for item in self.iterable:\n",
        "            yield item\n",
        "        if self.desc:\n",
        "            print(f\"{self.desc}: done\")\n",
        "    def update(self, n=1):\n",
        "        pass\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "try:\n",
        "    from tqdm.auto import tqdm as _tqdm\n",
        "    def progress(iterable, **kwargs):\n",
        "        defaults = {\n",
        "            \"mininterval\": float(os.environ.get(\"TQDM_MININTERVAL\", \"1.0\")),\n",
        "            \"maxinterval\": float(os.environ.get(\"TQDM_MAX_INTERVAL\", \"5.0\")),\n",
        "            \"leave\": False,\n",
        "            \"dynamic_ncols\": True,\n",
        "            \"position\": 0,\n",
        "            \"smoothing\": 0.0,\n",
        "        }\n",
        "        defaults.update(kwargs)\n",
        "        return _tqdm(iterable, **defaults)\n",
        "except Exception:\n",
        "    progress = SimpleProgress\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipeline = OCRPipeline(\n",
        "    output_dir=str(OUTPUT_DIR),\n",
        "    temp_dir=str(TEMP_DIR),\n",
        "    dpi=DPI,\n",
        "    enable_preprocessing=ENABLE_PREPROCESSING,\n",
        "    auto_detect=PROCESSING_MODE == \"auto\",\n",
        "    extract_images=EXTRACT_IMAGES,\n",
        "    analyze_layout=PRESERVE_LAYOUT,\n",
        "    extract_tables=EXTRACT_TABLES,\n",
        "    use_llm_correction=USE_LLM_CORRECTION\n",
        ")\n",
        "\n",
        "# Configure parallel workers\n",
        "pipeline.max_workers = NUM_WORKERS\n",
        "\n",
        "print(f\"Pipeline initialized with {NUM_WORKERS} workers\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"Temporary directory: {TEMP_DIR}\")\n",
        "print(\"\\nReady to process PDFs!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b80dfaa8",
      "metadata": {
        "id": "b80dfaa8"
      },
      "source": [
        "## 6. Process PDFs with Progress Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c3f1c41",
      "metadata": {
        "id": "5c3f1c41"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING OCR PROCESSING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pdf_files = sorted(INPUT_DIR.glob(\"*.pdf\"))\n",
        "\n",
        "if not pdf_files:\n",
        "    print(\"\\nNo PDF files found in input directory!\")\n",
        "    print(\"Please upload PDFs first (see section 3).\")\n",
        "else:\n",
        "    total_files = len(pdf_files)\n",
        "    print(f\"\\nFound {total_files} PDF(s) to process\")\n",
        "    print(f\"Processing Mode: {PROCESSING_MODE}\")\n",
        "    print(f\"Using {NUM_WORKERS} parallel workers\\n\")\n",
        "\n",
        "    # Process with a single progress iterator to avoid duplicate bars\n",
        "    start_time = time.time()\n",
        "    results = []\n",
        "\n",
        "    for pdf_path in progress(pdf_files, total=len(pdf_files), desc=\"Processing PDFs\"):\n",
        "        try:\n",
        "            file_start = time.time()\n",
        "\n",
        "            # Process the PDF\n",
        "            output_path = pipeline.process_pdf(\n",
        "                pdf_path,\n",
        "                mode=PROCESSING_MODE if PROCESSING_MODE != \"auto\" else None\n",
        "            )\n",
        "\n",
        "            file_duration = time.time() - file_start\n",
        "\n",
        "            # Get output file size\n",
        "            output_size = output_path.stat().st_size / (1024*1024)\n",
        "\n",
        "            results.append({\n",
        "                \"input\": pdf_path.name,\n",
        "                \"output\": output_path.name,\n",
        "                \"status\": \"success\",\n",
        "                \"duration_seconds\": file_duration,\n",
        "                \"output_size_mb\": output_size\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            results.append({\n",
        "                \"input\": pdf_path.name,\n",
        "                \"status\": \"failed\",\n",
        "                \"error\": error_msg\n",
        "            })\n",
        "\n",
        "    total_duration = time.time() - start_time\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n\\n\" + \"=\"*60)\n",
        "    print(\"PROCESSING COMPLETE - SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    successful = sum(1 for r in results if r[\"status\"] == \"success\")\n",
        "    failed = sum(1 for r in results if r[\"status\"] == \"failed\")\n",
        "\n",
        "    print(f\"\\nTotal Files: {total_files}\")\n",
        "    print(f\"Successful: {successful}\")\n",
        "    print(f\"Failed: {failed}\")\n",
        "    print(f\"Total Processing Time: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\")\n",
        "    print(f\"Average Time per File: {total_duration/total_files:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gdH4K0F-R-e4",
      "metadata": {
        "id": "gdH4K0F-R-e4"
      },
      "outputs": [],
      "source": [
        "# Helper function to convert datetime objects in a dictionary to ISO format strings\n",
        "def convert_datetime_to_iso(obj):\n",
        "    if isinstance(obj, datetime):\n",
        "        return obj.isoformat()\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: convert_datetime_to_iso(v) for k, v in obj.items()}\n",
        "    if isinstance(obj, list):\n",
        "        return [convert_datetime_to_iso(elem) for elem in obj]\n",
        "    return obj\n",
        "\n",
        "# Save results to JSON\n",
        "metrics = pipeline.metrics.get_metrics_summary() if hasattr(pipeline, 'metrics') else {}\n",
        "\n",
        "# Convert datetime objects in metrics to ISO format strings\n",
        "metrics_serializable = convert_datetime_to_iso(metrics)\n",
        "\n",
        "results_json = OUTPUT_DIR / \"processing_results.json\"\n",
        "with open(results_json, 'w', encoding='utf-8') as f:\n",
        "    json.dump({\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"total_files\": total_files,\n",
        "        \"successful\": successful,\n",
        "        \"failed\": failed,\n",
        "        \"total_duration_seconds\": total_duration,\n",
        "        \"workers_used\": NUM_WORKERS,\n",
        "        \"results\": results,\n",
        "        \"metrics\": metrics_serializable # Use the serializable metrics\n",
        "    }, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\nResults saved to: processing_results.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4a0f1a0",
      "metadata": {
        "id": "e4a0f1a0"
      },
      "source": [
        "## 7. Process Manually Corrected Markdown: Add Section Breaks & Convert to Word\n",
        "\n",
        "After manually editing the markdown file, use this section to:\n",
        "1. Add `</break>` tags at the end of each level-2 heading section\n",
        "2. Convert the processed markdown to a Word document with proper formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2850f57c",
      "metadata": {
        "id": "2850f57c"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"UPLOAD MANUALLY CORRECTED MARKDOWN FILE\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nUpload your manually corrected .md file to:\")\n",
        "print(\"1. Add </break> tags at the end of level-2 heading sections\")\n",
        "print(\"2. Convert to Word document with proper formatting\\n\")\n",
        "\n",
        "# Upload markdown file\n",
        "uploaded_md_manual = files.upload()\n",
        "\n",
        "# Store uploaded markdown files\n",
        "MANUAL_MD_DIR = Path(WORK_DIR) / \"manual_markdown\"\n",
        "MANUAL_MD_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "manual_md_files = []\n",
        "for filename, data in uploaded_md_manual.items():\n",
        "    if filename.lower().endswith('.md'):\n",
        "        md_path = MANUAL_MD_DIR / filename\n",
        "        with open(md_path, 'wb') as f:\n",
        "            f.write(data)\n",
        "        file_size_kb = md_path.stat().st_size / 1024\n",
        "        print(f\"{filename} ({file_size_kb:.2f} KB) uploaded successfully\")\n",
        "        manual_md_files.append(md_path)\n",
        "    else:\n",
        "        print(f\"Skipped {filename} (not a markdown file)\")\n",
        "\n",
        "print(f\"\\nTotal markdown files uploaded: {len(manual_md_files)}\")\n",
        "\n",
        "# Process each markdown file: add </break> tags\n",
        "if manual_md_files:\n",
        "    # Initialize markdown processor\n",
        "    from modules.markdown_processor import MarkdownProcessor\n",
        "    md_processor = MarkdownProcessor(use_llm_correction=False)  # No LLM correction, just add breaks\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PROCESSING MARKDOWN: ADDING SECTION BREAKS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create output directory\n",
        "    PROCESSED_MD_DIR = OUTPUT_DIR / \"processed_markdown\"\n",
        "    PROCESSED_MD_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "    processed_files = []\n",
        "\n",
        "    for md_file in manual_md_files:\n",
        "        print(f\"\\nProcessing: {md_file.name}\")\n",
        "\n",
        "        try:\n",
        "            # Read markdown content\n",
        "            with open(md_file, 'r', encoding='utf-8') as f:\n",
        "                markdown_content = f.read()\n",
        "\n",
        "            # Insert section breaks\n",
        "            processed_content = md_processor.insert_section_breaks(markdown_content)\n",
        "\n",
        "            # Save processed markdown\n",
        "            output_filename = md_file.stem + \"_with_breaks.md\"\n",
        "            output_md_path = PROCESSED_MD_DIR / output_filename\n",
        "\n",
        "            with open(output_md_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(processed_content)\n",
        "\n",
        "            print(f\"Added </break> tags\")\n",
        "            print(f\"  Saved to: {output_filename}\")\n",
        "\n",
        "            processed_files.append(output_md_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {md_file.name}: {e}\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processed {len(processed_files)} markdown file(s)\")\n",
        "    print(f\"Output directory: {PROCESSED_MD_DIR}\")\n",
        "\n",
        "    # Now convert processed markdown to Word\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CONVERTING PROCESSED MARKDOWN TO WORD\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    from modules.exporter import WordExporter\n",
        "    exporter = WordExporter()\n",
        "\n",
        "    # Create output directory for Word files\n",
        "    WORD_OUTPUT_DIR = OUTPUT_DIR / \"manual_word_output\"\n",
        "    WORD_OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "    conversion_results = []\n",
        "\n",
        "    for md_file in processed_files:\n",
        "        print(f\"\\nConverting: {md_file.name}\")\n",
        "\n",
        "        try:\n",
        "            # Read processed markdown content\n",
        "            with open(md_file, 'r', encoding='utf-8') as f:\n",
        "                markdown_content = f.read()\n",
        "\n",
        "            # Output path\n",
        "            output_filename = md_file.stem.replace('_with_breaks', '') + \".docx\"\n",
        "            output_path = WORD_OUTPUT_DIR / output_filename\n",
        "\n",
        "            # Look for associated images\n",
        "            pdf_name = md_file.stem.replace('_ocr_results_with_breaks', '')\n",
        "            possible_image_dirs = [\n",
        "                OUTPUT_DIR / \"extracted_images\",\n",
        "            ]\n",
        "\n",
        "            images_list = []\n",
        "            for img_dir in possible_image_dirs:\n",
        "                if img_dir.exists():\n",
        "                    print(f\"  Found image directory: {img_dir}\")\n",
        "                    for img_file in sorted(img_dir.glob(f\"{pdf_name}_img_*.png\")):\n",
        "                        img_id = img_file.stem\n",
        "                        images_list.append({\n",
        "                            'image_id': img_id,\n",
        "                            'file_path': str(img_file),\n",
        "                            'width': 800,\n",
        "                            'height': 600\n",
        "                        })\n",
        "                    if images_list:\n",
        "                        print(f\"  Loaded {len(images_list)} images\")\n",
        "                        break\n",
        "\n",
        "            # Convert markdown to Word\n",
        "            exporter.markdown_to_word(\n",
        "                markdown_content,\n",
        "                output_path=str(output_path),\n",
        "                images=images_list if images_list else None\n",
        "            )\n",
        "\n",
        "            output_size_kb = output_path.stat().st_size / 1024\n",
        "\n",
        "            print(f\"SUCCESS: {output_filename}\")\n",
        "            print(f\"  Size: {output_size_kb:.2f} KB\")\n",
        "            print(f\"  Images: {len(images_list)}\")\n",
        "\n",
        "            conversion_results.append({\n",
        "                \"input\": md_file.name,\n",
        "                \"output\": output_filename,\n",
        "                \"status\": \"success\",\n",
        "                \"size_kb\": output_size_kb,\n",
        "                \"images_count\": len(images_list)\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— ERROR: {e}\")\n",
        "            conversion_results.append({\n",
        "                \"input\": md_file.name,\n",
        "                \"status\": \"failed\",\n",
        "                \"error\": str(e)\n",
        "            })\n",
        "\n",
        "    # Print final summary\n",
        "    print(f\"\\n\\n{'='*60}\")\n",
        "    print(\"PROCESSING COMPLETE - SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    successful = sum(1 for r in conversion_results if r[\"status\"] == \"success\")\n",
        "    failed = sum(1 for r in conversion_results if r[\"status\"] == \"failed\")\n",
        "\n",
        "    print(f\"\\nMarkdown Files Processed: {len(manual_md_files)}\")\n",
        "    print(f\"Word Conversions Successful: {successful}\")\n",
        "    print(f\"Word Conversions Failed: {failed}\")\n",
        "\n",
        "    if successful > 0:\n",
        "        print(f\"\\nOutput files:\")\n",
        "        print(f\"  - Processed Markdown: {PROCESSED_MD_DIR}\")\n",
        "        print(f\"  - Word Documents: {WORD_OUTPUT_DIR}\")\n",
        "        print(\"\\nGenerated files:\")\n",
        "        for result in conversion_results:\n",
        "            if result[\"status\"] == \"success\":\n",
        "                print(f\"  - {result['output']} ({result['size_kb']:.1f} KB, {result['images_count']} images)\")\n",
        "else:\n",
        "    print(\"\\nNo markdown files uploaded. Please upload files first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1d5e0db",
      "metadata": {
        "id": "b1d5e0db"
      },
      "source": [
        "## 9. Download All Output Files as ZIP\n",
        "\n",
        "Download the entire output directory (including all results, converted files, images, etc.) as a single ZIP archive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7795cd8",
      "metadata": {
        "id": "d7795cd8"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ZIPPING OUTPUT DIRECTORY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "zip_path = str(OUTPUT_DIR) + \".zip\"\n",
        "\n",
        "# Remove old zip if exists\n",
        "if os.path.exists(zip_path):\n",
        "    os.remove(zip_path)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, dirs, files_in_dir in os.walk(OUTPUT_DIR):\n",
        "        for file in files_in_dir:\n",
        "            abs_path = os.path.join(root, file)\n",
        "            rel_path = os.path.relpath(abs_path, OUTPUT_DIR)\n",
        "            zipf.write(abs_path, arcname=rel_path)\n",
        "            print(f\"Added: {rel_path}\")\n",
        "\n",
        "zip_size_mb = os.path.getsize(zip_path) / (1024*1024)\n",
        "print(f\"\\nZIP archive created: {zip_path} ({zip_size_mb:.2f} MB)\")\n",
        "print(\"Downloading...\")\n",
        "files.download(zip_path)\n",
        "print(\"Download started! Check your browser's download folder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7518b81",
      "metadata": {
        "id": "e7518b81"
      },
      "source": [
        "## 10. Cleanup (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86ab9471",
      "metadata": {
        "id": "86ab9471"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLEANUP OPTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def cleanup_directory(directory: Path, description: str):\n",
        "    \"\"\"Clean up a directory and show freed space\"\"\"\n",
        "    if directory.exists():\n",
        "        total_size = sum(f.stat().st_size for f in directory.rglob('*') if f.is_file())\n",
        "        shutil.rmtree(directory)\n",
        "        size_mb = total_size / (1024*1024)\n",
        "        print(f\"Cleaned up {description}: freed {size_mb:.2f} MB\")\n",
        "    else:\n",
        "        print(f\"{description} not found\")\n",
        "\n",
        "# Uncomment to cleanup temporary files\n",
        "cleanup_directory(TEMP_DIR, \"Temporary files\")\n",
        "\n",
        "# Uncomment to cleanup input files (after processing)\n",
        "cleanup_directory(INPUT_DIR, \"Input files\")\n",
        "\n",
        "# Uncomment to cleanup everything (keep this commented unless you're sure!)\n",
        "# cleanup_directory(OUTPUT_DIR, \"Output files\")\n",
        "# shutil.rmtree(Path(WORK_DIR) / \"repo\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
