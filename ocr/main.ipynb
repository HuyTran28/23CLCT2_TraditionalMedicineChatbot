{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e5d63e",
   "metadata": {},
   "source": [
    "## 1. Setup: Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Create working directory\n",
    "WORK_DIR = \"/content/traditional_medicine_ocr\"\n",
    "Path(WORK_DIR).mkdir(exist_ok=True)\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "print(f\"Working directory: {WORK_DIR}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLONING REPOSITORY...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clone the repository\n",
    "!git clone https://github.com/HuyTran28/23CLCT2_TraditionalMedicineChatbot.git repo\n",
    "\n",
    "print(\"\\nRepository cloned successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3c038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INSTALLING DEPENDENCIES (GPU OPTIMIZED)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Install pip packages with GPU support\n",
    "!pip install --upgrade pip setuptools wheel -q\n",
    "\n",
    "# Install core dependencies\n",
    "!pip install -q numpy>=1.26.4\n",
    "!pip install -q Pillow>=10.1.0,<11.0.0\n",
    "!pip install -q matplotlib>=3.8.0\n",
    "!pip install -q PyMuPDF>=1.23.8\n",
    "!pip install -q python-docx>=1.1.0\n",
    "!pip install -q pdf2docx>=0.5.8\n",
    "\n",
    "# Install OCR and NLP dependencies\n",
    "!pip install -q underthesea>=6.0.0\n",
    "!pip install -q pyvi>=0.1.1\n",
    "!pip install -q opencv-contrib-python>=4.8.0.74\n",
    "!pip install -q opencv-python>=4.8.0.74\n",
    "\n",
    "# Install marker-pdf (GPU enabled on Colab)\n",
    "print(\"\\nInstalling marker-pdf (this may take a few minutes)...\")\n",
    "!pip install -q marker-pdf>=0.2.0\n",
    "\n",
    "# Verify GPU availability for marker-pdf\n",
    "print(\"\\nVerifying GPU availability...\")\n",
    "!nvidia-smi --query-gpu=name --format=csv,noheader\n",
    "\n",
    "print(\"\\nAll dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e115c49a",
   "metadata": {},
   "source": [
    "## 2. Setup Environment & Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9eba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Any\n",
    "import shutil\n",
    "import json\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "from multiprocessing import cpu_count\n",
    "import zipfile\n",
    "import time\n",
    "\n",
    "# Add OCR modules to path\n",
    "ocr_module_path = Path(WORK_DIR) / \"repo\" / \"ocr\"\n",
    "sys.path.insert(0, str(ocr_module_path))\n",
    "sys.path.insert(0, str(ocr_module_path.parent))\n",
    "\n",
    "# Import pipeline and modules\n",
    "from modules.pipeline import OCRPipeline\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create directories\n",
    "INPUT_DIR = Path(WORK_DIR) / \"input\"\n",
    "OUTPUT_DIR = Path(WORK_DIR) / \"output\"\n",
    "TEMP_DIR = Path(WORK_DIR) / \"temp\"\n",
    "\n",
    "INPUT_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "TEMP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Environment setup complete\")\n",
    "print(f\"Input directory: {INPUT_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"CPU Count: {cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff6ac6",
   "metadata": {},
   "source": [
    "## 3. Upload Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"UPLOAD PDF FILES\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nClick 'Choose Files' to select PDF(s) from your computer.\")\n",
    "print(\"You can upload multiple PDFs at once.\")\n",
    "print(\"\\nSupported: Single or batch processing of PDFs (including ~300+ pages)\\n\")\n",
    "\n",
    "uploaded_files = files.upload()\n",
    "\n",
    "# Move uploaded files to input directory\n",
    "uploaded_count = 0\n",
    "for filename, data in uploaded_files.items():\n",
    "    if filename.lower().endswith('.pdf'):\n",
    "        input_path = INPUT_DIR / filename\n",
    "        with open(input_path, 'wb') as f:\n",
    "            f.write(data)\n",
    "        file_size_mb = input_path.stat().st_size / (1024*1024)\n",
    "        print(f\"{filename} ({file_size_mb:.2f} MB) uploaded successfully\")\n",
    "        uploaded_count += 1\n",
    "    else:\n",
    "        print(f\"Skipped {filename} (not a PDF)\")\n",
    "\n",
    "print(f\"\\nTotal PDFs uploaded: {uploaded_count}\")\n",
    "\n",
    "# List uploaded files\n",
    "pdf_files = sorted(INPUT_DIR.glob(\"*.pdf\"))\n",
    "print(f\"\\nPDF files ready for processing:\")\n",
    "for pdf in pdf_files:\n",
    "    print(f\"  - {pdf.name} ({pdf.stat().st_size / (1024*1024):.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3648187",
   "metadata": {},
   "source": [
    "## 4. Configure Processing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f17173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PROCESSING CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Number of parallel workers for processing\n",
    "# NUM_WORKERS = cpu_count()\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "PROCESSING_MODE = \"scan\"\n",
    "\n",
    "# PDF conversion settings\n",
    "DPI = 300  # Resolution for PDF to image conversion\n",
    "EXTRACT_IMAGES = True  # Extract images from PDF\n",
    "PRESERVE_LAYOUT = True  # Preserve document layout/structure\n",
    "EXTRACT_TABLES = True  # Extract and process tables\n",
    "ENABLE_PREPROCESSING = True  # Preprocess scanned pages\n",
    "\n",
    "# LLM correction \n",
    "USE_LLM_CORRECTION = True\n",
    "\n",
    "# Display configuration\n",
    "print(\"=\"*60)\n",
    "print(\"PROCESSING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of Workers (CPU cores): {NUM_WORKERS}\")\n",
    "print(f\"Processing Mode: {PROCESSING_MODE}\")\n",
    "print(f\"DPI: {DPI}\")\n",
    "print(f\"Extract Images: {EXTRACT_IMAGES}\")\n",
    "print(f\"Preserve Layout: {PRESERVE_LAYOUT}\")\n",
    "print(f\"Extract Tables: {EXTRACT_TABLES}\")\n",
    "print(f\"Enable Preprocessing: {ENABLE_PREPROCESSING}\")\n",
    "print(f\"LLM Correction: {USE_LLM_CORRECTION}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf1ad44",
   "metadata": {},
   "source": [
    "## 5. Initialize OCR Pipeline with Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf74a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nInitializing OCR Pipeline...\\n\")\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipeline = OCRPipeline(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    temp_dir=str(TEMP_DIR),\n",
    "    dpi=DPI,\n",
    "    enable_preprocessing=ENABLE_PREPROCESSING,\n",
    "    auto_detect=PROCESSING_MODE == \"auto\",\n",
    "    extract_images=EXTRACT_IMAGES,\n",
    "    analyze_layout=PRESERVE_LAYOUT,\n",
    "    extract_tables=EXTRACT_TABLES,\n",
    "    use_llm_correction=USE_LLM_CORRECTION\n",
    ")\n",
    "\n",
    "# Configure parallel workers\n",
    "pipeline.max_workers = NUM_WORKERS\n",
    "\n",
    "print(f\"Pipeline initialized with {NUM_WORKERS} workers\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Temporary directory: {TEMP_DIR}\")\n",
    "print(\"\\nReady to process PDFs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80dfaa8",
   "metadata": {},
   "source": [
    "## 6. Process PDFs with Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING OCR PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pdf_files = sorted(INPUT_DIR.glob(\"*.pdf\"))\n",
    "\n",
    "if not pdf_files:\n",
    "    print(\"\\nNo PDF files found in input directory!\")\n",
    "    print(\"Please upload PDFs first (see section 3).\")\n",
    "else:\n",
    "    total_files = len(pdf_files)\n",
    "    print(f\"\\nFound {total_files} PDF(s) to process\")\n",
    "    print(f\"Processing mode: {PROCESSING_MODE}\")\n",
    "    print(f\"Using {NUM_WORKERS} parallel workers\\n\")\n",
    "    \n",
    "    # Process with parallelism\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    \n",
    "    for idx, pdf_path in enumerate(pdf_files, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing file {idx}/{total_files}: {pdf_path.name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"File size: {pdf_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        try:\n",
    "            file_start = time.time()\n",
    "            \n",
    "            # Process the PDF\n",
    "            output_path = pipeline.process_pdf(\n",
    "                pdf_path,\n",
    "                mode=PROCESSING_MODE if PROCESSING_MODE != \"auto\" else None\n",
    "            )\n",
    "            \n",
    "            file_duration = time.time() - file_start\n",
    "            \n",
    "            # Get output file size\n",
    "            output_size = output_path.stat().st_size / (1024*1024)\n",
    "            \n",
    "            print(f\"\\nSUCCESS: {output_path.name}\")\n",
    "            print(f\"Output size: {output_size:.2f} MB\")\n",
    "            print(f\"Processing time: {file_duration:.2f} seconds\")\n",
    "            \n",
    "            results.append({\n",
    "                \"input\": pdf_path.name,\n",
    "                \"output\": output_path.name,\n",
    "                \"status\": \"success\",\n",
    "                \"duration_seconds\": file_duration,\n",
    "                \"output_size_mb\": output_size\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"\\nERROR: {error_msg}\")\n",
    "            \n",
    "            results.append({\n",
    "                \"input\": pdf_path.name,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": error_msg\n",
    "            })\n",
    "    \n",
    "    total_duration = time.time() - start_time\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING COMPLETE - SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    successful = sum(1 for r in results if r[\"status\"] == \"success\")\n",
    "    failed = sum(1 for r in results if r[\"status\"] == \"failed\")\n",
    "    \n",
    "    print(f\"\\nTotal Files: {total_files}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Total Processing Time: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\")\n",
    "    print(f\"Average Time per File: {total_duration/total_files:.2f} seconds\")\n",
    "    \n",
    "    # Save results to JSON\n",
    "    metrics = pipeline.metrics.get_metrics_summary() if hasattr(pipeline, 'metrics') else {}\n",
    "    results_json = OUTPUT_DIR / \"processing_results.json\"\n",
    "    with open(results_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"total_files\": total_files,\n",
    "            \"successful\": successful,\n",
    "            \"failed\": failed,\n",
    "            \"total_duration_seconds\": total_duration,\n",
    "            \"workers_used\": NUM_WORKERS,\n",
    "            \"results\": results,\n",
    "            \"metrics\": metrics\n",
    "        }, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nResults saved to: processing_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67f456",
   "metadata": {},
   "source": [
    "## 7. Manual Post-Processing: Upload Corrected Markdown\n",
    "\n",
    "After reviewing and manually correcting the markdown output, upload your corrected `.md` file here to convert it to a Word document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980572f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"UPLOAD CORRECTED MARKDOWN FILE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nUpload your manually corrected .md file for conversion to Word.\\n\")\n",
    "\n",
    "# Upload markdown file\n",
    "uploaded_md = files.upload()\n",
    "\n",
    "# Store uploaded markdown files\n",
    "MARKDOWN_DIR = Path(WORK_DIR) / \"markdown_uploads\"\n",
    "MARKDOWN_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "md_files = []\n",
    "for filename, data in uploaded_md.items():\n",
    "    if filename.lower().endswith('.md'):\n",
    "        md_path = MARKDOWN_DIR / filename\n",
    "        with open(md_path, 'wb') as f:\n",
    "            f.write(data)\n",
    "        file_size_kb = md_path.stat().st_size / 1024\n",
    "        print(f\"{filename} ({file_size_kb:.2f} KB) uploaded successfully\")\n",
    "        md_files.append(md_path)\n",
    "    else:\n",
    "        print(f\"Skipped {filename} (not a markdown file)\")\n",
    "\n",
    "print(f\"\\nTotal markdown files uploaded: {len(md_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e56a08",
   "metadata": {},
   "source": [
    "## 8. Convert Markdown to Word Document\n",
    "\n",
    "Convert the uploaded markdown files to professional Word documents with proper formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONVERTING MARKDOWN TO WORD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not md_files:\n",
    "    print(\"\\nNo markdown files to convert. Please upload files first (see section 7).\")\n",
    "else:\n",
    "    # Create output directory for converted files\n",
    "    CONVERTED_DIR = OUTPUT_DIR / \"converted_markdown\"\n",
    "    CONVERTED_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Initialize the exporter\n",
    "    from modules.exporter import WordExporter\n",
    "    exporter = WordExporter()\n",
    "    \n",
    "    conversion_results = []\n",
    "    \n",
    "    for md_file in md_files:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Converting: {md_file.name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Read markdown content\n",
    "            with open(md_file, 'r', encoding='utf-8') as f:\n",
    "                markdown_content = f.read()\n",
    "            \n",
    "            # Output path\n",
    "            output_filename = md_file.stem + \".docx\"\n",
    "            output_path = CONVERTED_DIR / output_filename\n",
    "            \n",
    "            # Look for associated images in the extracted_images folder\n",
    "            # Try to find images from the original OCR output\n",
    "            pdf_name = md_file.stem.replace('_corrected', '').replace('_edited', '')\n",
    "            possible_image_dirs = [\n",
    "                OUTPUT_DIR / \"extracted_images\" / pdf_name,\n",
    "                OUTPUT_DIR / \"extracted_images\",\n",
    "            ]\n",
    "            \n",
    "            images_list = []\n",
    "            for img_dir in possible_image_dirs:\n",
    "                if img_dir.exists():\n",
    "                    print(f\"Found image directory: {img_dir}\")\n",
    "                    # Load images from directory\n",
    "                    for img_file in sorted(img_dir.glob(\"img_*.png\")):\n",
    "                        img_id = img_file.stem\n",
    "                        images_list.append({\n",
    "                            'image_id': img_id,\n",
    "                            'file_path': str(img_file),\n",
    "                            'width': 800,  # Default width\n",
    "                            'height': 600   # Default height\n",
    "                        })\n",
    "                    if images_list:\n",
    "                        print(f\"Loaded {len(images_list)} images\")\n",
    "                        break\n",
    "            \n",
    "            # Convert markdown to Word\n",
    "            print(f\"Converting to Word document...\")\n",
    "            exporter.markdown_to_word(\n",
    "                markdown_content,\n",
    "                output_path=str(output_path),\n",
    "                images=images_list if images_list else None\n",
    "            )\n",
    "            \n",
    "            # Get file size\n",
    "            output_size_kb = output_path.stat().st_size / 1024\n",
    "            \n",
    "            print(f\"\\nSUCCESS: {output_filename}\")\n",
    "            print(f\"  Output: {output_path}\")\n",
    "            print(f\"  Size: {output_size_kb:.2f} KB\")\n",
    "            print(f\"  Images included: {len(images_list)}\")\n",
    "            \n",
    "            conversion_results.append({\n",
    "                \"input\": md_file.name,\n",
    "                \"output\": output_filename,\n",
    "                \"status\": \"success\",\n",
    "                \"size_kb\": output_size_kb,\n",
    "                \"images_count\": len(images_list)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"\\nERROR: {error_msg}\")\n",
    "            conversion_results.append({\n",
    "                \"input\": md_file.name,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": error_msg\n",
    "            })\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"CONVERSION COMPLETE - SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    successful = sum(1 for r in conversion_results if r[\"status\"] == \"success\")\n",
    "    failed = sum(1 for r in conversion_results if r[\"status\"] == \"failed\")\n",
    "    \n",
    "    print(f\"\\nTotal Files: {len(md_files)}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    \n",
    "    if successful > 0:\n",
    "        print(f\"\\nConverted files saved to: {CONVERTED_DIR}\")\n",
    "        print(\"\\nConverted files:\")\n",
    "        for result in conversion_results:\n",
    "            if result[\"status\"] == \"success\":\n",
    "                print(f\"  - {result['output']} ({result['size_kb']:.1f} KB, {result['images_count']} images)\")\n",
    "    \n",
    "    # Save conversion results\n",
    "    results_json = CONVERTED_DIR / \"conversion_results.json\"\n",
    "    with open(results_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"total_files\": len(md_files),\n",
    "            \"successful\": successful,\n",
    "            \"failed\": failed,\n",
    "            \"results\": conversion_results\n",
    "        }, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5e0db",
   "metadata": {},
   "source": [
    "## 9. Download All Output Files as ZIP\n",
    "\n",
    "Download the entire output directory (including all results, converted files, images, etc.) as a single ZIP archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7795cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ZIPPING OUTPUT DIRECTORY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "zip_path = str(OUTPUT_DIR) + \".zip\"\n",
    "\n",
    "# Remove old zip if exists\n",
    "if os.path.exists(zip_path):\n",
    "    os.remove(zip_path)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files_in_dir in os.walk(OUTPUT_DIR):\n",
    "        for file in files_in_dir:\n",
    "            abs_path = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(abs_path, OUTPUT_DIR)\n",
    "            zipf.write(abs_path, arcname=rel_path)\n",
    "            print(f\"Added: {rel_path}\")\n",
    "\n",
    "zip_size_mb = os.path.getsize(zip_path) / (1024*1024)\n",
    "print(f\"\\nZIP archive created: {zip_path} ({zip_size_mb:.2f} MB)\")\n",
    "print(\"Downloading...\")\n",
    "files.download(zip_path)\n",
    "print(\"Download started! Check your browser's download folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7518b81",
   "metadata": {},
   "source": [
    "## 10. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLEANUP OPTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def cleanup_directory(directory: Path, description: str):\n",
    "    \"\"\"Clean up a directory and show freed space\"\"\"\n",
    "    if directory.exists():\n",
    "        total_size = sum(f.stat().st_size for f in directory.rglob('*') if f.is_file())\n",
    "        shutil.rmtree(directory)\n",
    "        size_mb = total_size / (1024*1024)\n",
    "        print(f\"Cleaned up {description}: freed {size_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"{description} not found\")\n",
    "\n",
    "# Uncomment to cleanup temporary files\n",
    "cleanup_directory(TEMP_DIR, \"Temporary files\")\n",
    "\n",
    "# Uncomment to cleanup input files (after processing)\n",
    "cleanup_directory(INPUT_DIR, \"Input files\")\n",
    "\n",
    "# Uncomment to cleanup everything (keep this commented unless you're sure!)\n",
    "# cleanup_directory(OUTPUT_DIR, \"Output files\")\n",
    "# shutil.rmtree(Path(WORK_DIR) / \"repo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
