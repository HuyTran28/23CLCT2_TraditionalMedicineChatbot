{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "631f175a",
   "metadata": {},
   "source": [
    "# 1. Setup Google Colab GPU Environment\n",
    "Configure the runtime to use GPU acceleration in Google Colab. Check GPU availability and display GPU information using `nvidia-smi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e00514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import os\n",
    "os.system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7a81b",
   "metadata": {},
   "source": [
    "# 2. Install Required Libraries\n",
    "Install necessary packages for PDF processing, GPU computation, and parallelism such as PyPDF2, pdfplumber, CUDA-compatible libraries, and parallel processing tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d3a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository and install dependencies\n",
    "!git clone https://github.com/HuyTran28/23CLCT2_TraditionalMedicineChatbot.git /content/repo\n",
    "%cd /content/repo/ocr\n",
    "\n",
    "# Install required libraries\n",
    "!pip install PyPDF2 pdfplumber ray torch tensorflow tqdm pytesseract pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc80d7",
   "metadata": {},
   "source": [
    "# 3. Configure GPU Access and Memory\n",
    "Set up TensorFlow or PyTorch to utilize GPU, configure memory growth to prevent OOM errors, and verify GPU is accessible to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow GPU setup\n",
    "import tensorflow as tf\n",
    "try:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"TensorFlow is using GPU: {gpus}\")\n",
    "    else:\n",
    "        print(\"No GPU found for TensorFlow.\")\n",
    "except Exception as e:\n",
    "    print(f\"TensorFlow GPU setup error: {e}\")\n",
    "\n",
    "# PyTorch GPU setup\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"PyTorch is using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU found for PyTorch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a4a08",
   "metadata": {},
   "source": [
    "# 4. Load and Preprocess PDF\n",
    "Load the 300-page PDF document, split into chunks or pages, and prepare data structures for parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c36ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the repo modules to path\n",
    "import sys\n",
    "sys.path.insert(0, '/content/repo/ocr')\n",
    "\n",
    "# Upload PDF file to Colab\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "pdf_path = next(iter(uploaded))\n",
    "\n",
    "# Load PDF and split into pages\n",
    "import pdfplumber\n",
    "pages = []\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for i in range(len(pdf.pages)):\n",
    "        pages.append(pdf.pages[i])\n",
    "print(f\"Loaded {len(pages)} pages from PDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1709d663",
   "metadata": {},
   "source": [
    "# 5. Implement Parallel Processing with GPU\n",
    "Use multiprocessing, concurrent.futures, or Ray to parallelize PDF processing tasks across GPU cores. Distribute pages or chunks across available compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d0758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules from the cloned repository\n",
    "from modules.ocr_engine import OCREngine\n",
    "from modules.pipeline import Pipeline\n",
    "from modules.exporter import Exporter\n",
    "import ray\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Initialize OCR engine with GPU support\n",
    "ocr_engine = OCREngine(use_gpu=True)\n",
    "\n",
    "@ray.remote\n",
    "def process_page_with_pipeline(page_num, page):\n",
    "    \"\"\"Process a page using the repository's OCR pipeline\"\"\"\n",
    "    try:\n",
    "        # Extract text using the OCR engine\n",
    "        text = ocr_engine.process_image(page)\n",
    "        return page_num, text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing page {page_num}: {e}\")\n",
    "        return page_num, \"\"\n",
    "\n",
    "# Run parallel processing\n",
    "results = ray.get([process_page_with_pipeline.remote(i, pages[i]) for i in range(len(pages))])\n",
    "results.sort(key=lambda x: x[0])\n",
    "texts = [text for _, text in results]\n",
    "print(f\"Processed {len(texts)} pages in parallel using the pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea5b83f",
   "metadata": {},
   "source": [
    "# 6. Optimize Memory Usage for Large Files\n",
    "Implement batch processing, streaming techniques, and memory-efficient data structures to handle large PDFs without exceeding Colab's memory limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing example for memory efficiency\n",
    "batch_size = 50\n",
    "batched_results = []\n",
    "for start in range(0, len(pages), batch_size):\n",
    "    batch = pages[start:start+batch_size]\n",
    "    batch_results = ray.get([process_page.remote(i+start, page) for i, page in enumerate(batch)])\n",
    "    batch_results.sort(key=lambda x: x[0])\n",
    "    batched_results.extend(batch_results)\n",
    "    print(f\"Processed batch {start//batch_size + 1} of {len(pages)//batch_size + 1}\")\n",
    "texts = [text for _, text in batched_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9f2552",
   "metadata": {},
   "source": [
    "# 7. Monitor GPU Performance\n",
    "Track GPU utilization, memory consumption, and processing speed using GPU monitoring tools and custom metrics during pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU usage and processing speed\n",
    "import time\n",
    "start_time = time.time()\n",
    "os.system('nvidia-smi')\n",
    "# (Run your pipeline here)\n",
    "end_time = time.time()\n",
    "print(f\"Total processing time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5f2c9",
   "metadata": {},
   "source": [
    "# 8. Run Pipeline and Benchmark Results\n",
    "Execute the complete pipeline, measure execution time, compare CPU vs GPU performance, and document optimization improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline and benchmark\n",
    "import time\n",
    "start = time.time()\n",
    "# Run batch processing\n",
    "end = time.time()\n",
    "print(f\"GPU parallel processing time: {end-start:.2f} seconds\")\n",
    "\n",
    "# Save results\n",
    "with open('ocr_results.txt', 'w', encoding='utf-8') as f:\n",
    "    for i, text in enumerate(texts):\n",
    "        f.write(f\"Page {i+1}:\\n{text}\\n\\n\")\n",
    "print(\"OCR results saved to ocr_results.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
