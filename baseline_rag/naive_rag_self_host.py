import os
import shutil
import requests
from typing import Any, Optional
from pathlib import Path

try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

from llama_index.core import (
    VectorStoreIndex, 
    SimpleDirectoryReader, 
    Settings, 
    StorageContext, 
    load_index_from_storage
)
from llama_index.core.node_parser import MarkdownNodeParser
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# --- IMPORT MODULE QUAN TR·ªåNG ƒê·ªÇ T·∫†O CUSTOM LLM ---
from llama_index.core.llms import CustomLLM, CompletionResponse, LLMMetadata
from llama_index.core.llms.callbacks import llm_completion_callback

# ==============================================================================
# 1. CLASS K·∫æT N·ªêI V·ªöI GOOGLE COLAB (RemoteColabLLM)
# ==============================================================================
class RemoteColabLLM(CustomLLM):
    api_url: str = ""
    context_window: int = 4096
    num_output: int = 512
    model_name: str = "colab-gpu-model"

    def __init__(self, api_url: str, **kwargs):
        super().__init__(api_url=api_url, **kwargs)

    @property
    def metadata(self) -> LLMMetadata:
        return LLMMetadata(
            context_window=self.context_window,
            num_output=self.num_output,
            model_name=self.model_name,
        )

    @llm_completion_callback()
    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        # C·∫•u tr√∫c payload kh·ªõp v·ªõi file colab_llm_server.ipynb c·ªßa b·∫°n
        endpoint = f"{self.api_url}/v1/complete"
        headers = {"Content-Type": "application/json"}
        
        payload = {
            "prompt": prompt,
            "max_new_tokens": self.num_output,
            "temperature": kwargs.get("temperature", 0.1),
        }
        
        try:
            print(f"üì° Sending request to Colab: {endpoint}...")
            response = requests.post(endpoint, json=payload, headers=headers, timeout=120)
            response.raise_for_status()
            
            # X·ª≠ l√Ω k·∫øt qu·∫£ tr·∫£ v·ªÅ t·ª´ Notebook
            # D·ª±a v√†o snippet notebook, server tr·∫£ v·ªÅ text ho·∫∑c JSON
            try:
                data = response.json()
                # ∆Øu ti√™n l·∫•y field 'text' ho·∫∑c 'content', n·∫øu kh√¥ng c√≥ th√¨ l·∫•y c·∫£ c·ª•c
                text = data.get("text", data.get("content", str(data)))
            except:
                text = response.text
                
            return CompletionResponse(text=text)
            
        except requests.exceptions.RequestException as e:
            return CompletionResponse(text=f"Error connecting to Colab API: {e}")

    def stream_complete(self, prompt: str, **kwargs: Any):
        # Notebook m·∫´u c·ªßa b·∫°n d√πng /v1/complete (kh√¥ng stream), n√™n ta gi·∫£ l·∫≠p stream
        yield self.complete(prompt, **kwargs)

# ==============================================================================
# 2. CLASS BASELINE RAG (ƒê√É S·ª¨A ƒê·ªÇ D√ôNG REMOTE LLM)
# ==============================================================================
class NaiveMedicalRAG:
    def __init__(self, file_paths, persist_dir="./baseline_storage", colab_url=None):
        """
        Baseline RAG Hybrid:
        - Embedding: Ch·∫°y Local CPU (BAAI/bge-m3) - V√¨ n√≥ nh·∫π.
        - LLM: Ch·∫°y Remote GPU (Google Colab) - V√¨ n√≥ n·∫∑ng.
        """
        self.persist_dir = persist_dir
        
        # 1. C·∫•u h√¨nh LLM (K·∫øt n·ªëi Colab)
        # ∆Øu ti√™n URL truy·ªÅn v√†o, n·∫øu kh√¥ng th√¨ l·∫•y t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
        api_base = colab_url or os.getenv("LLM_API_BASE")
        
        if not api_base:
            print("‚ö†Ô∏è C·∫¢NH B√ÅO: Ch∆∞a c√≥ URL Colab. H·ªá th·ªëng s·∫Ω kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.")
            print("üëâ H√£y set bi·∫øn m√¥i tr∆∞·ªùng LLM_API_BASE ho·∫∑c truy·ªÅn colab_url v√†o.")
            # Fallback dummy ƒë·ªÉ kh√¥ng crash l√∫c init, nh∆∞ng s·∫Ω l·ªói l√∫c query
            self.llm = None
        else:
            # X√≥a d·∫•u / ·ªü cu·ªëi n·∫øu c√≥
            api_base = api_base.rstrip("/")
            print(f"--- ƒêang k·∫øt n·ªëi v·ªõi Colab LLM t·∫°i: {api_base} ---")
            self.llm = RemoteColabLLM(api_url=api_base)
            Settings.llm = self.llm

        # 2. C·∫•u h√¨nh Embedding (Ch·∫°y Local CPU)
        print("--- ƒêang load model Embedding BAAI/bge-m3 (CPU Mode)... ---")
        self.embed_model = HuggingFaceEmbedding(
            model_name="BAAI/bge-m3",
            device="cpu", 
            embed_batch_size=4 # Batch nh·ªè ƒë·ªÉ nh·∫π m√°y
        )
        Settings.embed_model = self.embed_model
        Settings.context_window = 4096

        # 3. Load ho·∫∑c T·∫°o Index
        if os.path.exists(self.persist_dir):
            print(f"--- [Baseline] Loading Index t·ª´ ƒëƒ©a: {self.persist_dir} ---")
            try:
                storage_context = StorageContext.from_defaults(persist_dir=self.persist_dir)
                self.index = load_index_from_storage(storage_context)
            except Exception as e:
                print(f"L·ªói load index: {e}. ƒêang t·∫°o l·∫°i...")
                if os.path.exists(self.persist_dir):
                    shutil.rmtree(self.persist_dir)
                self.create_index(file_paths)
        else:
            print(f"--- [Baseline] T·∫°o m·ªõi Index... ---")
            self.create_index(file_paths)

        self.query_engine = self.index.as_query_engine(similarity_top_k=3)
        print("--- Baseline RAG ƒë√£ s·∫µn s√†ng ---")

    def create_index(self, file_paths):
        if not file_paths:
            raise ValueError("Danh s√°ch file input ƒëang tr·ªëng!")
        
        print(f"ƒêang ƒë·ªçc {len(file_paths)} file t√†i li·ªáu...")
        documents = SimpleDirectoryReader(input_files=file_paths).load_data()
        
        print("ƒêang parse document...")
        parser = MarkdownNodeParser()
        nodes = parser.get_nodes_from_documents(documents)
        
        print(f"ƒê√£ t·∫°o {len(nodes)} nodes. ƒêang t·∫°o embedding...")
        self.index = VectorStoreIndex(nodes, show_progress=True)
        
        self.index.storage_context.persist(persist_dir=self.persist_dir)
        print(f"ƒê√£ l∆∞u index xu·ªëng: {self.persist_dir}")

    def query(self, question: str, return_full=False):
        if not self.llm:
            return "L·ªói: Ch∆∞a k·∫øt n·ªëi ƒë∆∞·ª£c v·ªõi Colab LLM Server."
            
        response = self.query_engine.query(question)
        if return_full:
            return response
        return str(response)

# ==============================================================================
# 3. CH·∫†Y TH·ª¨
# ==============================================================================
if __name__ == "__main__":
    # --- C·∫§U H√åNH URL COLAB C·ª¶A B·∫†N T·∫†I ƒê√ÇY ---
    # V√≠ d·ª•: "https://a1b2-34-123-45-67.ngrok-free.app"
    COLAB_URL = "D√ÅN_URL_NGROK_C·ª¶A_B·∫†N_V√ÄO_ƒê√ÇY" 
    
    # N·∫øu ch·∫°y d√≤ng l·ªánh th√¨ set: set LLM_API_BASE=https://...
    if COLAB_URL == "D√ÅN_URL_NGROK_C·ª¶A_B·∫†N_V√ÄO_ƒê√ÇY":
        COLAB_URL = os.getenv("LLM_API_BASE")

    DATA_DIR = Path(__file__).resolve().parent.parent / "data" / "raw"
    # S·ª≠a ƒë∆∞·ªùng d·∫´n file test cho ph√π h·ª£p
    test_files = [
        str(DATA_DIR / "cay-rau-lam-thuoc" / "cay-rau-lam-thuoc.md")
    ]
    
    # Ki·ªÉm tra file t·ªìn t·∫°i kh√¥ng ƒë·ªÉ tr√°nh l·ªói
    if not os.path.exists(test_files[0]):
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file m·∫´u: {test_files[0]}")
        print("Vui l√≤ng s·ª≠a list `test_files` trong code.")
    else:
        try:
            bot = NaiveMedicalRAG(test_files, colab_url=COLAB_URL)
            
            q = "C√¢y ·ªõt c√≥ c√¥ng d·ª•ng g√¨?"
            print(f"\n‚ùì C√¢u h·ªèi: {q}")
            res = bot.query(q)
            print(f"üí° C√¢u tr·∫£ l·ªùi t·ª´ Colab:\n{res}")
            
        except Exception as e:
            print(f"‚ùå L·ªói: {e}")